{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "RNN_LSTM_GRU_languageModelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshudaur/TensorFlowProjects/blob/master/RNN_LSTM_GRU_languageModelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuIPwr6s-mRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import time\n",
        "import pandas as pd\n",
        "import copy\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E59CZMs2-mRV",
        "colab_type": "code",
        "outputId": "8c5f6c4b-5111-4a34-b013-82fe67fc7110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 prepare_data2.py the-king-james-bible.txt shake \\\\n\\\\n+ -m 500"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-02 09:10:43.568354: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 24607 sequences...\n",
            "Longest sequence is 2552 characters. If this seems unreasonable, consider using the maxlen argument!\n",
            "Removing sequences longer than 500 characters...\n",
            "24057 sequences remaining.\n",
            "Longest remaining sequence has length 499.\n",
            "Removing length-0 sequences...\n",
            "24057 sequences remaining.\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n",
            "Serialized 23000 sequences...\n",
            "Serialized 23100 sequences...\n",
            "Serialized 23200 sequences...\n",
            "Serialized 23300 sequences...\n",
            "Serialized 23400 sequences...\n",
            "Serialized 23500 sequences...\n",
            "Serialized 23600 sequences...\n",
            "Serialized 23700 sequences...\n",
            "Serialized 23800 sequences...\n",
            "Serialized 23900 sequences...\n",
            "Serialized 24000 sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG8HKu8V-mRZ",
        "colab_type": "code",
        "outputId": "a42dda33-6559-41f2-fd93-ebbcd7168344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from prepare_data2 import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"shake.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "# data = data.map(lambda x: parse_seq(x, 200))\n",
        "\n",
        "# Now no fixed length sequence\n",
        "data = data.map(parse_seq)\n",
        "# data = data.padded_batch(batch_size, (-1,), drop_remainder=True)\n",
        "\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"shake_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print('Vocab : {}\\n'.format(vocab))\n",
        "print('Vocab Size : {}\\n'.format(vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab : {'I': 3, '9': 4, 'w': 5, 'n': 6, 'K': 7, 'b': 8, '8': 9, '.': 10, 'o': 11, '1': 12, '6': 13, ' ': 14, '0': 15, '7': 16, 'N': 17, 'O': 18, 'Y': 19, '\\n': 20, 'L': 21, 'A': 22, 'C': 23, 'V': 24, 'P': 25, 'x': 26, 'W': 27, 'G': 28, 'd': 29, 'r': 30, 'u': 31, 't': 32, 'p': 33, '4': 34, 'z': 35, 'f': 36, 'E': 37, 'j': 38, 'i': 39, 'Z': 40, '2': 41, '5': 42, '?': 43, 'S': 44, 'g': 45, 'c': 46, 'U': 47, 'J': 48, '*': 49, 'M': 50, 'T': 51, '!': 52, 'h': 53, ')': 54, 'l': 55, ';': 56, 'B': 57, 'F': 58, 'Q': 59, 'm': 60, 'a': 61, 'R': 62, 's': 63, 'q': 64, '(': 65, '-': 66, ',': 67, ':': 68, 'v': 69, \"'\": 70, '\\ufeff': 71, 'k': 72, 'D': 73, '3': 74, 'y': 75, 'e': 76, 'H': 77, '<PAD>': 0, '<S>': 1, '</S>': 2}\n",
            "\n",
            "Vocab Size : 78\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8nKYEgx-mRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lambda function to convert data to one hot encoding format for each character\n",
        "# using Lambda function of keras to add the one_hot to sequential model\n",
        "one_hot_encode = tf.keras.layers.Lambda(lambda data: tf.one_hot(indices=data,depth=vocab_size));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "982p3Jde-mRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_model():\n",
        "    # lstm sequential model\n",
        "    model = tf.keras.Sequential()\n",
        "    # The LSTM input layer should be 3D. The dimensions are : samples, time steps, and features.\n",
        "    model.add(tf.keras.layers.LSTM(512, activation='tanh', recurrent_activation='sigmoid', use_bias=True,\n",
        "        kernel_initializer='glorot_uniform', return_sequences=True, stateful=True))\n",
        "    model.add(tf.keras.layers.Dense(vocab_size))\n",
        "    # Provide the inputs to the LSTM layer.(samples=batch_size, time steps=None, features=vocab_size)\n",
        "    model.build([batch_size , None , vocab_size])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buhDrrmyEHRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru_model():\n",
        "    # lstm sequential model\n",
        "    model = tf.keras.Sequential()\n",
        "    # The LSTM input layer should be 3D. The dimensions are : samples, time steps, and features.\n",
        "    model.add(tf.keras.layers.GRU(512, activation='tanh', recurrent_activation='sigmoid', use_bias=True,\n",
        "        kernel_initializer='glorot_uniform', return_sequences=True, stateful=True))\n",
        "    model.add(tf.keras.layers.Dense(vocab_size))\n",
        "    model.build([batch_size , None , vocab_size])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gwDZWUtjPO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_RNN_model():\n",
        "    # lstm sequential model\n",
        "    model = tf.keras.Sequential()\n",
        "    # The LSTM input layer should be 3D. The dimensions are : samples, time steps, and features.\n",
        "    model.add(tf.keras.layers.SimpleRNN(512, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform',\n",
        "    recurrent_initializer='orthogonal', bias_initializer='zeros', return_sequences=True, stateful=True))\n",
        "    model.add(tf.keras.layers.Dense(vocab_size))\n",
        "    model.build([batch_size , None , vocab_size])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ8yDq6D-mRi",
        "colab_type": "code",
        "outputId": "933ac82b-61cc-4444-c134-88516a8a35df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# sequence_length = 200 #already given\n",
        "#each batch should be batch_size x seq_len x vocab_size\n",
        "batch_size=128\n",
        "\n",
        "test_data=data\n",
        "# test_data = test_data.batch(batch_size,drop_remainder=True)\n",
        "test_data = test_data.padded_batch(batch_size,(-1,),drop_remainder=True)\n",
        "print(\"Test data : \", test_data)\n",
        "\n",
        "# train_data = data.map(one_hot_encode)\n",
        "train_data=data\n",
        "# train_data = data.map(one_hot_encode)\n",
        "train_data = train_data.padded_batch(batch_size,(-1,),drop_remainder=True)\n",
        "\n",
        "# train_data = train_data.batch(batch_size,drop_remainder=True)\n",
        "print(\"Train data : \",train_data)\n",
        "\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data :  <PaddedBatchDataset shapes: (128, None), types: tf.int32>\n",
            "Train data :  <PaddedBatchDataset shapes: (128, None), types: tf.int32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7rBpi4t-mRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(rnn_model, train_data, test_data, epochs):\n",
        "    model = rnn_model\n",
        "    model_loss = 0\n",
        "    for epoch in range(epochs):    \n",
        "        for batch_num,(x, y) in enumerate(zip(train_data,test_data)):\n",
        "            x_train = x[:,:-1]\n",
        "            y_label = y[:,1:]\n",
        "            # counting actual elements as the padded values are zero.\n",
        "            nonzero_elements = tf.math.count_nonzero(x, 1)\n",
        "            # Subtracting 1 as the last elememt is not used as input \n",
        "            nonzero_elements -= 1\n",
        "            # 2D mask batch x time\n",
        "            masking = tf.sequence_mask(nonzero_elements, dtype=tf.float32)\n",
        "            # print('x_train : {}\\n'.format(x_train))\n",
        "            with tf.GradientTape() as tape:\n",
        "                # one hot encoding for input sequence characters\n",
        "                x_train = model(one_hot_encode(x_train))\n",
        "                # (batch x time) loss and mask multiplication with softmax_cross_entropy\n",
        "                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=x_train, labels=y_label)\n",
        "                loss = tf.reduce_sum(loss * masking)\n",
        "                # cast to float32 from int64\n",
        "                real_elements = tf.cast(nonzero_elements, tf.float32)\n",
        "                # using reduced_sum instead of reduced_mean\n",
        "                loss /= tf.reduce_sum(real_elements)\n",
        "            if not batch_num % 100:  \n",
        "                print(\"Epoch ::\",epoch, \" loss :: \",loss)  \n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "#             metric(y_label, x_train) \n",
        "\n",
        "        # Display metrics at the end of each epoch.\n",
        "#         accuracy = metric.result()    \n",
        "#         print('Training acc over epoch' , epoch,' : ',(float(accuracy)))\n",
        "        # Reset training metrics at the end of each epoch\n",
        "#         metric.reset_states()\n",
        "        model.reset_states()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9J9IvNF-mRp",
        "colab_type": "code",
        "outputId": "e86f00b1-ed31-4c98-954b-cf052bc807f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "# model variables : recommended 512 hidden units (batch size 128 and Adam optimizer, 20 or so )\n",
        "train_model(lstm_model(),train_data,test_data,epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  multiple                  1210368   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  40014     \n",
            "=================================================================\n",
            "Total params: 1,250,382\n",
            "Trainable params: 1,250,382\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch :: 0  loss ::  tf.Tensor(4.362029, shape=(), dtype=float32)\n",
            "Epoch :: 0  loss ::  tf.Tensor(2.3848474, shape=(), dtype=float32)\n",
            "Epoch :: 1  loss ::  tf.Tensor(2.0934634, shape=(), dtype=float32)\n",
            "Epoch :: 1  loss ::  tf.Tensor(2.0286229, shape=(), dtype=float32)\n",
            "Epoch :: 2  loss ::  tf.Tensor(1.9367489, shape=(), dtype=float32)\n",
            "Epoch :: 2  loss ::  tf.Tensor(1.9310391, shape=(), dtype=float32)\n",
            "Epoch :: 3  loss ::  tf.Tensor(1.8654876, shape=(), dtype=float32)\n",
            "Epoch :: 3  loss ::  tf.Tensor(1.8709047, shape=(), dtype=float32)\n",
            "Epoch :: 4  loss ::  tf.Tensor(1.8147076, shape=(), dtype=float32)\n",
            "Epoch :: 4  loss ::  tf.Tensor(1.8299718, shape=(), dtype=float32)\n",
            "Epoch :: 5  loss ::  tf.Tensor(1.774194, shape=(), dtype=float32)\n",
            "Epoch :: 5  loss ::  tf.Tensor(1.7967387, shape=(), dtype=float32)\n",
            "Epoch :: 6  loss ::  tf.Tensor(1.7409351, shape=(), dtype=float32)\n",
            "Epoch :: 6  loss ::  tf.Tensor(1.7712278, shape=(), dtype=float32)\n",
            "Epoch :: 7  loss ::  tf.Tensor(1.7139484, shape=(), dtype=float32)\n",
            "Epoch :: 7  loss ::  tf.Tensor(1.745912, shape=(), dtype=float32)\n",
            "Epoch :: 8  loss ::  tf.Tensor(1.6904529, shape=(), dtype=float32)\n",
            "Epoch :: 8  loss ::  tf.Tensor(1.7249643, shape=(), dtype=float32)\n",
            "Epoch :: 9  loss ::  tf.Tensor(1.6702485, shape=(), dtype=float32)\n",
            "Epoch :: 9  loss ::  tf.Tensor(1.7069037, shape=(), dtype=float32)\n",
            "Epoch :: 10  loss ::  tf.Tensor(1.6524957, shape=(), dtype=float32)\n",
            "Epoch :: 10  loss ::  tf.Tensor(1.6908474, shape=(), dtype=float32)\n",
            "Epoch :: 11  loss ::  tf.Tensor(1.6367629, shape=(), dtype=float32)\n",
            "Epoch :: 11  loss ::  tf.Tensor(1.6761705, shape=(), dtype=float32)\n",
            "Epoch :: 12  loss ::  tf.Tensor(1.6224056, shape=(), dtype=float32)\n",
            "Epoch :: 12  loss ::  tf.Tensor(1.6626351, shape=(), dtype=float32)\n",
            "Epoch :: 13  loss ::  tf.Tensor(1.609412, shape=(), dtype=float32)\n",
            "Epoch :: 13  loss ::  tf.Tensor(1.6500587, shape=(), dtype=float32)\n",
            "Epoch :: 14  loss ::  tf.Tensor(1.5975267, shape=(), dtype=float32)\n",
            "Epoch :: 14  loss ::  tf.Tensor(1.6388236, shape=(), dtype=float32)\n",
            "Epoch :: 15  loss ::  tf.Tensor(1.58664, shape=(), dtype=float32)\n",
            "Epoch :: 15  loss ::  tf.Tensor(1.6287298, shape=(), dtype=float32)\n",
            "Epoch :: 16  loss ::  tf.Tensor(1.5767229, shape=(), dtype=float32)\n",
            "Epoch :: 16  loss ::  tf.Tensor(1.6195433, shape=(), dtype=float32)\n",
            "Epoch :: 17  loss ::  tf.Tensor(1.567609, shape=(), dtype=float32)\n",
            "Epoch :: 17  loss ::  tf.Tensor(1.6112096, shape=(), dtype=float32)\n",
            "Epoch :: 18  loss ::  tf.Tensor(1.5591114, shape=(), dtype=float32)\n",
            "Epoch :: 18  loss ::  tf.Tensor(1.6036129, shape=(), dtype=float32)\n",
            "Epoch :: 19  loss ::  tf.Tensor(1.5512141, shape=(), dtype=float32)\n",
            "Epoch :: 19  loss ::  tf.Tensor(1.5965884, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZeQ5yBx-mRu",
        "colab_type": "code",
        "outputId": "a98b06f6-03f0-4890-dc58-6ed003c79503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "# model variables : recommended 512 hidden units (batch size 128 and Adam optimizer, 20 or so )\n",
        "train_model(simple_RNN_model(),train_data,test_data,epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       multiple                  302592    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  40014     \n",
            "=================================================================\n",
            "Total params: 342,606\n",
            "Trainable params: 342,606\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch :: 0  loss ::  tf.Tensor(4.382275, shape=(), dtype=float32)\n",
            "Epoch :: 0  loss ::  tf.Tensor(2.8626513, shape=(), dtype=float32)\n",
            "Epoch :: 1  loss ::  tf.Tensor(2.4816494, shape=(), dtype=float32)\n",
            "Epoch :: 1  loss ::  tf.Tensor(2.3797395, shape=(), dtype=float32)\n",
            "Epoch :: 2  loss ::  tf.Tensor(2.2638786, shape=(), dtype=float32)\n",
            "Epoch :: 2  loss ::  tf.Tensor(2.2185423, shape=(), dtype=float32)\n",
            "Epoch :: 3  loss ::  tf.Tensor(2.1526468, shape=(), dtype=float32)\n",
            "Epoch :: 3  loss ::  tf.Tensor(2.1383965, shape=(), dtype=float32)\n",
            "Epoch :: 4  loss ::  tf.Tensor(2.0894508, shape=(), dtype=float32)\n",
            "Epoch :: 4  loss ::  tf.Tensor(2.08746, shape=(), dtype=float32)\n",
            "Epoch :: 5  loss ::  tf.Tensor(2.0442085, shape=(), dtype=float32)\n",
            "Epoch :: 5  loss ::  tf.Tensor(2.0505526, shape=(), dtype=float32)\n",
            "Epoch :: 6  loss ::  tf.Tensor(2.0116003, shape=(), dtype=float32)\n",
            "Epoch :: 6  loss ::  tf.Tensor(2.020531, shape=(), dtype=float32)\n",
            "Epoch :: 7  loss ::  tf.Tensor(1.9823889, shape=(), dtype=float32)\n",
            "Epoch :: 7  loss ::  tf.Tensor(1.9988402, shape=(), dtype=float32)\n",
            "Epoch :: 8  loss ::  tf.Tensor(1.9515219, shape=(), dtype=float32)\n",
            "Epoch :: 8  loss ::  tf.Tensor(1.9757528, shape=(), dtype=float32)\n",
            "Epoch :: 9  loss ::  tf.Tensor(1.9328549, shape=(), dtype=float32)\n",
            "Epoch :: 9  loss ::  tf.Tensor(1.9540893, shape=(), dtype=float32)\n",
            "Epoch :: 10  loss ::  tf.Tensor(1.9138421, shape=(), dtype=float32)\n",
            "Epoch :: 10  loss ::  tf.Tensor(1.9414102, shape=(), dtype=float32)\n",
            "Epoch :: 11  loss ::  tf.Tensor(1.8928798, shape=(), dtype=float32)\n",
            "Epoch :: 11  loss ::  tf.Tensor(1.9310575, shape=(), dtype=float32)\n",
            "Epoch :: 12  loss ::  tf.Tensor(1.8728447, shape=(), dtype=float32)\n",
            "Epoch :: 12  loss ::  tf.Tensor(1.9113979, shape=(), dtype=float32)\n",
            "Epoch :: 13  loss ::  tf.Tensor(1.8574733, shape=(), dtype=float32)\n",
            "Epoch :: 13  loss ::  tf.Tensor(1.885962, shape=(), dtype=float32)\n",
            "Epoch :: 14  loss ::  tf.Tensor(1.8468245, shape=(), dtype=float32)\n",
            "Epoch :: 14  loss ::  tf.Tensor(1.87225, shape=(), dtype=float32)\n",
            "Epoch :: 15  loss ::  tf.Tensor(1.8336287, shape=(), dtype=float32)\n",
            "Epoch :: 15  loss ::  tf.Tensor(1.8607978, shape=(), dtype=float32)\n",
            "Epoch :: 16  loss ::  tf.Tensor(1.8252655, shape=(), dtype=float32)\n",
            "Epoch :: 16  loss ::  tf.Tensor(1.8521655, shape=(), dtype=float32)\n",
            "Epoch :: 17  loss ::  tf.Tensor(1.8134217, shape=(), dtype=float32)\n",
            "Epoch :: 17  loss ::  tf.Tensor(1.8471751, shape=(), dtype=float32)\n",
            "Epoch :: 18  loss ::  tf.Tensor(1.8076613, shape=(), dtype=float32)\n",
            "Epoch :: 18  loss ::  tf.Tensor(1.8380957, shape=(), dtype=float32)\n",
            "Epoch :: 19  loss ::  tf.Tensor(1.7997402, shape=(), dtype=float32)\n",
            "Epoch :: 19  loss ::  tf.Tensor(1.8244914, shape=(), dtype=float32)\n",
            "Epoch :: 20  loss ::  tf.Tensor(1.7868828, shape=(), dtype=float32)\n",
            "Epoch :: 20  loss ::  tf.Tensor(1.8168523, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS1thm1t-mRr",
        "colab_type": "code",
        "outputId": "d37a4d0c-6547-47cb-d9d1-6b535229674a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "# model variables : recommended 512 hidden units (batch size 128 and Adam optimizer, 20 or so )\n",
        "train_model(gru_model(),train_data,test_data,epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    multiple                  909312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  40014     \n",
            "=================================================================\n",
            "Total params: 949,326\n",
            "Trainable params: 949,326\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch :: 0  loss ::  tf.Tensor(4.360831, shape=(), dtype=float32)\n",
            "Epoch :: 0  loss ::  tf.Tensor(2.5551348, shape=(), dtype=float32)\n",
            "Epoch :: 1  loss ::  tf.Tensor(2.236947, shape=(), dtype=float32)\n",
            "Epoch :: 1  loss ::  tf.Tensor(2.1534698, shape=(), dtype=float32)\n",
            "Epoch :: 2  loss ::  tf.Tensor(2.0711107, shape=(), dtype=float32)\n",
            "Epoch :: 2  loss ::  tf.Tensor(2.0442955, shape=(), dtype=float32)\n",
            "Epoch :: 3  loss ::  tf.Tensor(1.9975369, shape=(), dtype=float32)\n",
            "Epoch :: 3  loss ::  tf.Tensor(1.9808009, shape=(), dtype=float32)\n",
            "Epoch :: 4  loss ::  tf.Tensor(1.9452388, shape=(), dtype=float32)\n",
            "Epoch :: 4  loss ::  tf.Tensor(1.9316989, shape=(), dtype=float32)\n",
            "Epoch :: 5  loss ::  tf.Tensor(1.9006355, shape=(), dtype=float32)\n",
            "Epoch :: 5  loss ::  tf.Tensor(1.8871877, shape=(), dtype=float32)\n",
            "Epoch :: 6  loss ::  tf.Tensor(1.8609072, shape=(), dtype=float32)\n",
            "Epoch :: 6  loss ::  tf.Tensor(1.8476412, shape=(), dtype=float32)\n",
            "Epoch :: 7  loss ::  tf.Tensor(1.8242087, shape=(), dtype=float32)\n",
            "Epoch :: 7  loss ::  tf.Tensor(1.8133165, shape=(), dtype=float32)\n",
            "Epoch :: 8  loss ::  tf.Tensor(1.7908947, shape=(), dtype=float32)\n",
            "Epoch :: 8  loss ::  tf.Tensor(1.7809768, shape=(), dtype=float32)\n",
            "Epoch :: 9  loss ::  tf.Tensor(1.7583503, shape=(), dtype=float32)\n",
            "Epoch :: 9  loss ::  tf.Tensor(1.7493607, shape=(), dtype=float32)\n",
            "Epoch :: 10  loss ::  tf.Tensor(1.7280859, shape=(), dtype=float32)\n",
            "Epoch :: 10  loss ::  tf.Tensor(1.7202413, shape=(), dtype=float32)\n",
            "Epoch :: 11  loss ::  tf.Tensor(1.6995034, shape=(), dtype=float32)\n",
            "Epoch :: 11  loss ::  tf.Tensor(1.6933475, shape=(), dtype=float32)\n",
            "Epoch :: 12  loss ::  tf.Tensor(1.6725557, shape=(), dtype=float32)\n",
            "Epoch :: 12  loss ::  tf.Tensor(1.6682343, shape=(), dtype=float32)\n",
            "Epoch :: 13  loss ::  tf.Tensor(1.6476192, shape=(), dtype=float32)\n",
            "Epoch :: 13  loss ::  tf.Tensor(1.6446164, shape=(), dtype=float32)\n",
            "Epoch :: 14  loss ::  tf.Tensor(1.6245006, shape=(), dtype=float32)\n",
            "Epoch :: 14  loss ::  tf.Tensor(1.6223532, shape=(), dtype=float32)\n",
            "Epoch :: 15  loss ::  tf.Tensor(1.6030849, shape=(), dtype=float32)\n",
            "Epoch :: 15  loss ::  tf.Tensor(1.6013441, shape=(), dtype=float32)\n",
            "Epoch :: 16  loss ::  tf.Tensor(1.5832037, shape=(), dtype=float32)\n",
            "Epoch :: 16  loss ::  tf.Tensor(1.581867, shape=(), dtype=float32)\n",
            "Epoch :: 17  loss ::  tf.Tensor(1.5647177, shape=(), dtype=float32)\n",
            "Epoch :: 17  loss ::  tf.Tensor(1.5636731, shape=(), dtype=float32)\n",
            "Epoch :: 18  loss ::  tf.Tensor(1.5475615, shape=(), dtype=float32)\n",
            "Epoch :: 18  loss ::  tf.Tensor(1.5467099, shape=(), dtype=float32)\n",
            "Epoch :: 19  loss ::  tf.Tensor(1.5316178, shape=(), dtype=float32)\n",
            "Epoch :: 19  loss ::  tf.Tensor(1.5308391, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLUHSmTFk338",
        "colab_type": "text"
      },
      "source": [
        "Repeating GRU layers to see if performance improves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoU_l1cbj88U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru_model_2():\n",
        "    # lstm sequential model\n",
        "    model = tf.keras.Sequential()\n",
        "    # The LSTM input layer should be 3D. The dimensions are : samples, time steps, and features.\n",
        "    model.add(tf.keras.layers.GRU(512, activation='tanh', recurrent_activation='sigmoid', use_bias=True,\n",
        "        kernel_initializer='glorot_uniform', return_sequences=True, stateful=True))\n",
        "    model.add(tf.keras.layers.GRU(512, activation='tanh', recurrent_activation='sigmoid', use_bias=True,\n",
        "        kernel_initializer='glorot_uniform', return_sequences=True, stateful=True))\n",
        "    model.add(tf.keras.layers.Dense(vocab_size))\n",
        "    model.build([batch_size , None , vocab_size])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9aLt76wlJEI",
        "colab_type": "code",
        "outputId": "fab35b5e-a7f1-4b98-d251-30eee27da7d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model variables : recommended 512 hidden units (batch size 128 and Adam optimizer, 20 or so )\n",
        "train_model(gru_model_2(),train_data,test_data,epochs=30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_2 (GRU)                  multiple                  909312    \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  multiple                  1575936   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  40014     \n",
            "=================================================================\n",
            "Total params: 2,525,262\n",
            "Trainable params: 2,525,262\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch :: 0  loss ::  tf.Tensor(4.3540874, shape=(), dtype=float32)\n",
            "Epoch :: 0  loss ::  tf.Tensor(3.3043652, shape=(), dtype=float32)\n",
            "Epoch :: 1  loss ::  tf.Tensor(6.034909, shape=(), dtype=float32)\n",
            "Epoch :: 1  loss ::  tf.Tensor(3.289881, shape=(), dtype=float32)\n",
            "Epoch :: 2  loss ::  tf.Tensor(3.1638744, shape=(), dtype=float32)\n",
            "Epoch :: 2  loss ::  tf.Tensor(3.0776439, shape=(), dtype=float32)\n",
            "Epoch :: 3  loss ::  tf.Tensor(2.8490252, shape=(), dtype=float32)\n",
            "Epoch :: 3  loss ::  tf.Tensor(2.8499234, shape=(), dtype=float32)\n",
            "Epoch :: 4  loss ::  tf.Tensor(2.6645892, shape=(), dtype=float32)\n",
            "Epoch :: 4  loss ::  tf.Tensor(2.674044, shape=(), dtype=float32)\n",
            "Epoch :: 5  loss ::  tf.Tensor(2.5015001, shape=(), dtype=float32)\n",
            "Epoch :: 5  loss ::  tf.Tensor(2.5605853, shape=(), dtype=float32)\n",
            "Epoch :: 6  loss ::  tf.Tensor(2.431598, shape=(), dtype=float32)\n",
            "Epoch :: 6  loss ::  tf.Tensor(2.485167, shape=(), dtype=float32)\n",
            "Epoch :: 7  loss ::  tf.Tensor(2.5930898, shape=(), dtype=float32)\n",
            "Epoch :: 7  loss ::  tf.Tensor(2.4179015, shape=(), dtype=float32)\n",
            "Epoch :: 8  loss ::  tf.Tensor(2.276132, shape=(), dtype=float32)\n",
            "Epoch :: 8  loss ::  tf.Tensor(2.369019, shape=(), dtype=float32)\n",
            "Epoch :: 9  loss ::  tf.Tensor(2.259643, shape=(), dtype=float32)\n",
            "Epoch :: 9  loss ::  tf.Tensor(2.3327289, shape=(), dtype=float32)\n",
            "Epoch :: 10  loss ::  tf.Tensor(2.1954367, shape=(), dtype=float32)\n",
            "Epoch :: 10  loss ::  tf.Tensor(2.3042822, shape=(), dtype=float32)\n",
            "Epoch :: 11  loss ::  tf.Tensor(2.1787567, shape=(), dtype=float32)\n",
            "Epoch :: 11  loss ::  tf.Tensor(2.2801213, shape=(), dtype=float32)\n",
            "Epoch :: 12  loss ::  tf.Tensor(2.142608, shape=(), dtype=float32)\n",
            "Epoch :: 12  loss ::  tf.Tensor(2.253616, shape=(), dtype=float32)\n",
            "Epoch :: 13  loss ::  tf.Tensor(2.1227129, shape=(), dtype=float32)\n",
            "Epoch :: 13  loss ::  tf.Tensor(2.2268076, shape=(), dtype=float32)\n",
            "Epoch :: 14  loss ::  tf.Tensor(2.0908124, shape=(), dtype=float32)\n",
            "Epoch :: 14  loss ::  tf.Tensor(2.2011645, shape=(), dtype=float32)\n",
            "Epoch :: 15  loss ::  tf.Tensor(2.080912, shape=(), dtype=float32)\n",
            "Epoch :: 15  loss ::  tf.Tensor(2.1826608, shape=(), dtype=float32)\n",
            "Epoch :: 16  loss ::  tf.Tensor(2.0584464, shape=(), dtype=float32)\n",
            "Epoch :: 16  loss ::  tf.Tensor(2.1670544, shape=(), dtype=float32)\n",
            "Epoch :: 17  loss ::  tf.Tensor(2.0651743, shape=(), dtype=float32)\n",
            "Epoch :: 17  loss ::  tf.Tensor(2.1525087, shape=(), dtype=float32)\n",
            "Epoch :: 18  loss ::  tf.Tensor(2.0419712, shape=(), dtype=float32)\n",
            "Epoch :: 18  loss ::  tf.Tensor(2.1402783, shape=(), dtype=float32)\n",
            "Epoch :: 19  loss ::  tf.Tensor(2.0740077, shape=(), dtype=float32)\n",
            "Epoch :: 19  loss ::  tf.Tensor(2.1278365, shape=(), dtype=float32)\n",
            "Epoch :: 20  loss ::  tf.Tensor(2.0020144, shape=(), dtype=float32)\n",
            "Epoch :: 20  loss ::  tf.Tensor(2.1175845, shape=(), dtype=float32)\n",
            "Epoch :: 21  loss ::  tf.Tensor(1.9969426, shape=(), dtype=float32)\n",
            "Epoch :: 21  loss ::  tf.Tensor(2.1078925, shape=(), dtype=float32)\n",
            "Epoch :: 22  loss ::  tf.Tensor(1.9800651, shape=(), dtype=float32)\n",
            "Epoch :: 22  loss ::  tf.Tensor(2.0986707, shape=(), dtype=float32)\n",
            "Epoch :: 23  loss ::  tf.Tensor(1.9730985, shape=(), dtype=float32)\n",
            "Epoch :: 23  loss ::  tf.Tensor(2.088592, shape=(), dtype=float32)\n",
            "Epoch :: 24  loss ::  tf.Tensor(1.9605937, shape=(), dtype=float32)\n",
            "Epoch :: 24  loss ::  tf.Tensor(2.0794916, shape=(), dtype=float32)\n",
            "Epoch :: 25  loss ::  tf.Tensor(1.9539618, shape=(), dtype=float32)\n",
            "Epoch :: 25  loss ::  tf.Tensor(2.071524, shape=(), dtype=float32)\n",
            "Epoch :: 26  loss ::  tf.Tensor(1.9475299, shape=(), dtype=float32)\n",
            "Epoch :: 26  loss ::  tf.Tensor(2.0643125, shape=(), dtype=float32)\n",
            "Epoch :: 27  loss ::  tf.Tensor(1.9473486, shape=(), dtype=float32)\n",
            "Epoch :: 27  loss ::  tf.Tensor(2.0572846, shape=(), dtype=float32)\n",
            "Epoch :: 28  loss ::  tf.Tensor(1.9375883, shape=(), dtype=float32)\n",
            "Epoch :: 28  loss ::  tf.Tensor(2.0508814, shape=(), dtype=float32)\n",
            "Epoch :: 29  loss ::  tf.Tensor(1.946491, shape=(), dtype=float32)\n",
            "Epoch :: 29  loss ::  tf.Tensor(2.045147, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RmhNSZIdwET",
        "colab_type": "text"
      },
      "source": [
        "Unable to make the text generation work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcYt2eEjlo8F",
        "colab_type": "code",
        "outputId": "d51fda86-29ad-42b4-d577-5ec81776066b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def sample(n_seq):        \n",
        "  for _ in range(n_seq):\n",
        "    model = gru_model_2()\n",
        "    char = 1      \n",
        "    txt = []\n",
        "    gen = tf.Variable([[tf.one_hot(char, vocab_size)]])\n",
        "    x_train = gen[:,:-1]\n",
        "    out = model(one_hot_encode(x_train)) #last executed GRU model\n",
        "    probs = tf.reshape(out),[-1]).numpy()\n",
        "    txt.append(np.argmax(probs))\n",
        "    #print(txt)\n",
        "      \n",
        "\n",
        "    print(\"\".join([ind_to_ch[ind] for ind in txt]).replace(\"</S>\", \"\\n\\n\"))\n",
        "        \n",
        "sample(100)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_34 (GRU)                 multiple                  909312    \n",
            "_________________________________________________________________\n",
            "gru_35 (GRU)                 multiple                  1575936   \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             multiple                  40014     \n",
            "=================================================================\n",
            "Total params: 2,525,262\n",
            "Trainable params: 2,525,262\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b9dc5b71737c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_to_ch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</S>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-b9dc5b71737c>\u001b[0m in \u001b[0;36msample\u001b[0;34m(n_seq)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#last executed GRU model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#probs = tf.reshape(out).numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwatch_accessed_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_variable_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreated_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatched_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c7684a2f4d56>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# lambda function to convert data to one hot encoding format for each character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# using Lambda function of keras to add the one_hot to sequential model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mone_hot_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, dtype, name)\u001b[0m\n\u001b[1;32m   4008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4009\u001b[0m     return gen_array_ops.one_hot(indices, depth, on_value, off_value, axis,\n\u001b[0;32m-> 4010\u001b[0;31m                                  name)\n\u001b[0m\u001b[1;32m   4011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[1;32m   6191\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6192\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6193\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6194\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6195\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find valid device for node.\nNode:{{node OneHot}}\nAll kernels registered for op OneHot :\n  device='XLA_CPU'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='XLA_CPU_JIT'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='XLA_GPU_JIT'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='XLA_GPU'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT32]; T in [DT_STRING]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT32]; T in [DT_HALF]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT32]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT64]\n  device='GPU'; TI in [DT_INT64]; T in [DT_INT64]\n  device='GPU'; TI in [DT_INT32]; T in [DT_INT64]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_INT64]\n  device='GPU'; TI in [DT_INT64]; T in [DT_INT32]\n  device='GPU'; TI in [DT_INT32]; T in [DT_INT32]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_INT32]\n  device='GPU'; TI in [DT_INT64]; T in [DT_BOOL]\n  device='GPU'; TI in [DT_INT32]; T in [DT_BOOL]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_BOOL]\n  device='GPU'; TI in [DT_INT64]; T in [DT_DOUBLE]\n  device='GPU'; TI in [DT_INT32]; T in [DT_DOUBLE]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_DOUBLE]\n  device='GPU'; TI in [DT_INT64]; T in [DT_FLOAT]\n  device='GPU'; TI in [DT_INT32]; T in [DT_FLOAT]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_FLOAT]\n  device='GPU'; TI in [DT_INT64]; T in [DT_HALF]\n  device='GPU'; TI in [DT_INT32]; T in [DT_HALF]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_HALF]\n [Op:OneHot]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xYUnEUnaOB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}