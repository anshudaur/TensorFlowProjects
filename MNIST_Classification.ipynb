{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnivQonuB/O8batZrg4RtK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshudaur/TensorFlowProjects/blob/master/MNIST_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nAZAWm3s_8U",
        "colab_type": "text"
      },
      "source": [
        "MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H2wg3m15J50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f51f2cac-34f5-4022-8217-14f49cb9c16b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import MNISTDataset\n",
        "tf.__version__ "
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVIbLlPt61EK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "eb8cecd6-bf07-42e1-a7ac-6d7395dd97a9"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "plt.imshow(train_images[0], cmap=\"Greys_r\")\n",
        "\n",
        "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
        "                    test_images.reshape([-1, 784]), test_labels,\n",
        "                    batch_size=128)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQTqmVD58DtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_steps = 1000\n",
        "learning_rate = 0.1\n",
        "# Outputs random values from a uniform distribution\n",
        "tf.random.uniform( shape=[],minval=-0.1,maxval=0.1, dtype=tf.dtypes.float32,seed=10)\n",
        "epochs = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_q9mk7ubgrH",
        "colab_type": "text"
      },
      "source": [
        "MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ets9zYCF9lWy",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "> 2 Hidden layers with RELU activation function : 11.35% test accuracy\n",
        "\n",
        "> Sigmoid activation function at the last layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZSMrTux9gGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "463d3625-d2ae-4f7c-c940-325c0b23aa0b"
      },
      "source": [
        "\n",
        "#random initialization of weights and bias tensors(multi-dimensional arrays)\n",
        "#weights and biases for first hidden layer\n",
        "W1 = tf.Variable(np.zeros([784, 128]), dtype=tf.float32) \n",
        "b1 = tf.Variable(np.zeros([128]), dtype=tf.float32)\n",
        "#weights and biases for second hidden layer\n",
        "W2 = tf.Variable(np.zeros([128, 64]), dtype=tf.float32) \n",
        "b2 = tf.Variable(np.zeros([64]), dtype=tf.float32)\n",
        "#weights and biases for output layer\n",
        "W3 = tf.Variable(np.zeros([64, 10]), dtype=tf.float32) \n",
        "b3 = tf.Variable(np.zeros([10]), dtype=tf.float32)\n",
        "\n",
        "for i in range(epochs):\n",
        "  for step in range(train_steps):\n",
        "      img_batch, lbl_batch = data.next_batch()\n",
        "      with tf.GradientTape() as tape:        \n",
        "          hidden_layer_1_input =  tf.add(tf.matmul(img_batch, W1), b1)\n",
        "          hidden_layer_1_output = tf.nn.relu(hidden_layer_1_input)\n",
        "          hidden_layer_2_input =  tf.add(tf.matmul(hidden_layer_1_output, W2),b2)\n",
        "          hidden_layer_2_output= tf.nn.relu(hidden_layer_2_input)\n",
        "          logits = tf.nn.sigmoid(tf.add(tf.matmul(hidden_layer_2_output, W3),b3))\n",
        "          xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "              logits=logits, labels=lbl_batch))\n",
        "      #automatic differentiation    \n",
        "      grads = tape.gradient(xent, [W1, b1, W2, b2, W3, b3])\n",
        "      W1.assign_sub(learning_rate * grads[0])\n",
        "      b1.assign_sub(learning_rate * grads[1])\n",
        "      W2.assign_sub(learning_rate * grads[2])\n",
        "      b2.assign_sub(learning_rate * grads[3])\n",
        "      W3.assign_sub(learning_rate * grads[4])\n",
        "      b3.assign_sub(learning_rate * grads[5])\n",
        "    \n",
        "      if not step % 100:\n",
        "          preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "          acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
        "                              tf.float32))\n",
        "          print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.3025851249694824 Accuracy: 0.0703125\n",
            "Loss: 2.302509307861328 Accuracy: 0.1171875\n",
            "Loss: 2.301858425140381 Accuracy: 0.140625\n",
            "Loss: 2.3027966022491455 Accuracy: 0.125\n",
            "Loss: 2.300851583480835 Accuracy: 0.140625\n",
            "Starting new epoch...\n",
            "Loss: 2.3020148277282715 Accuracy: 0.109375\n",
            "Loss: 2.3002467155456543 Accuracy: 0.109375\n",
            "Loss: 2.3031482696533203 Accuracy: 0.1015625\n",
            "Loss: 2.2954206466674805 Accuracy: 0.203125\n",
            "Loss: 2.3031325340270996 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.301443576812744 Accuracy: 0.1015625\n",
            "Loss: 2.2991583347320557 Accuracy: 0.140625\n",
            "Loss: 2.3034181594848633 Accuracy: 0.0703125\n",
            "Loss: 2.3019535541534424 Accuracy: 0.1015625\n",
            "Loss: 2.3025057315826416 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.29935359954834 Accuracy: 0.1171875\n",
            "Loss: 2.298353672027588 Accuracy: 0.140625\n",
            "Loss: 2.300729274749756 Accuracy: 0.109375\n",
            "Loss: 2.3012237548828125 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.303633689880371 Accuracy: 0.09375\n",
            "Loss: 2.3039042949676514 Accuracy: 0.125\n",
            "Loss: 2.300405502319336 Accuracy: 0.09375\n",
            "Loss: 2.302621364593506 Accuracy: 0.1171875\n",
            "Loss: 2.2971208095550537 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.2997238636016846 Accuracy: 0.09375\n",
            "Loss: 2.3014261722564697 Accuracy: 0.0859375\n",
            "Loss: 2.3009235858917236 Accuracy: 0.1015625\n",
            "Loss: 2.299525260925293 Accuracy: 0.1015625\n",
            "Loss: 2.3048038482666016 Accuracy: 0.0625\n",
            "Starting new epoch...\n",
            "Loss: 2.297293186187744 Accuracy: 0.140625\n",
            "Loss: 2.3055410385131836 Accuracy: 0.0546875\n",
            "Loss: 2.2880802154541016 Accuracy: 0.171875\n",
            "Loss: 2.3003532886505127 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.3083863258361816 Accuracy: 0.0625\n",
            "Loss: 2.3026041984558105 Accuracy: 0.09375\n",
            "Loss: 2.301685333251953 Accuracy: 0.109375\n",
            "Loss: 2.299614906311035 Accuracy: 0.125\n",
            "Loss: 2.302304744720459 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.312734365463257 Accuracy: 0.0546875\n",
            "Loss: 2.3031182289123535 Accuracy: 0.109375\n",
            "Loss: 2.2984983921051025 Accuracy: 0.140625\n",
            "Loss: 2.295841693878174 Accuracy: 0.109375\n",
            "Loss: 2.304007053375244 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.3077118396759033 Accuracy: 0.078125\n",
            "Loss: 2.2972967624664307 Accuracy: 0.1171875\n",
            "Loss: 2.2972214221954346 Accuracy: 0.15625\n",
            "Loss: 2.2978291511535645 Accuracy: 0.15625\n",
            "Starting new epoch...\n",
            "Loss: 2.297421932220459 Accuracy: 0.140625\n",
            "Loss: 2.2999486923217773 Accuracy: 0.09375\n",
            "Loss: 2.2993290424346924 Accuracy: 0.1171875\n",
            "Loss: 2.294480800628662 Accuracy: 0.140625\n",
            "Loss: 2.2986207008361816 Accuracy: 0.1328125\n",
            "Starting new epoch...\n",
            "Loss: 2.297532081604004 Accuracy: 0.125\n",
            "Loss: 2.3024072647094727 Accuracy: 0.1171875\n",
            "Loss: 2.308321475982666 Accuracy: 0.0546875\n",
            "Loss: 2.287559986114502 Accuracy: 0.1875\n",
            "Loss: 2.300858974456787 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.2967236042022705 Accuracy: 0.125\n",
            "Loss: 2.2980101108551025 Accuracy: 0.109375\n",
            "Loss: 2.2976417541503906 Accuracy: 0.1484375\n",
            "Loss: 2.301203966140747 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2932026386260986 Accuracy: 0.1484375\n",
            "Loss: 2.3037524223327637 Accuracy: 0.1015625\n",
            "Loss: 2.301422119140625 Accuracy: 0.125\n",
            "Loss: 2.300556182861328 Accuracy: 0.125\n",
            "Loss: 2.300124406814575 Accuracy: 0.0859375\n",
            "Starting new epoch...\n",
            "Loss: 2.3005807399749756 Accuracy: 0.078125\n",
            "Loss: 2.292755603790283 Accuracy: 0.15625\n",
            "Loss: 2.2986536026000977 Accuracy: 0.140625\n",
            "Loss: 2.299099922180176 Accuracy: 0.109375\n",
            "Loss: 2.293600559234619 Accuracy: 0.171875\n",
            "Starting new epoch...\n",
            "Loss: 2.297825813293457 Accuracy: 0.109375\n",
            "Loss: 2.304154872894287 Accuracy: 0.09375\n",
            "Loss: 2.307520866394043 Accuracy: 0.0703125\n",
            "Loss: 2.3008334636688232 Accuracy: 0.109375\n",
            "Loss: 2.2982048988342285 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.3036532402038574 Accuracy: 0.109375\n",
            "Loss: 2.3008341789245605 Accuracy: 0.109375\n",
            "Loss: 2.303431987762451 Accuracy: 0.140625\n",
            "Loss: 2.3028059005737305 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.296008348464966 Accuracy: 0.1640625\n",
            "Loss: 2.300753593444824 Accuracy: 0.09375\n",
            "Loss: 2.3011159896850586 Accuracy: 0.125\n",
            "Loss: 2.29901385307312 Accuracy: 0.1015625\n",
            "Loss: 2.302900791168213 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.303865671157837 Accuracy: 0.0859375\n",
            "Loss: 2.2926950454711914 Accuracy: 0.1328125\n",
            "Loss: 2.291761875152588 Accuracy: 0.1796875\n",
            "Loss: 2.3013622760772705 Accuracy: 0.0859375\n",
            "Loss: 2.301330089569092 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.296556234359741 Accuracy: 0.1484375\n",
            "Loss: 2.3002352714538574 Accuracy: 0.15625\n",
            "Loss: 2.2933602333068848 Accuracy: 0.171875\n",
            "Loss: 2.2992939949035645 Accuracy: 0.140625\n",
            "Starting new epoch...\n",
            "Loss: 2.2961888313293457 Accuracy: 0.1328125\n",
            "Loss: 2.3015363216400146 Accuracy: 0.109375\n",
            "Loss: 2.298004150390625 Accuracy: 0.1171875\n",
            "Loss: 2.3017115592956543 Accuracy: 0.09375\n",
            "Loss: 2.2989206314086914 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.3017654418945312 Accuracy: 0.109375\n",
            "Loss: 2.3002207279205322 Accuracy: 0.109375\n",
            "Loss: 2.296677589416504 Accuracy: 0.1328125\n",
            "Loss: 2.298790216445923 Accuracy: 0.125\n",
            "Loss: 2.2985334396362305 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.294726848602295 Accuracy: 0.125\n",
            "Loss: 2.3024978637695312 Accuracy: 0.1171875\n",
            "Loss: 2.3093857765197754 Accuracy: 0.078125\n",
            "Loss: 2.2978768348693848 Accuracy: 0.15625\n",
            "Starting new epoch...\n",
            "Loss: 2.304553985595703 Accuracy: 0.0859375\n",
            "Loss: 2.3039896488189697 Accuracy: 0.109375\n",
            "Loss: 2.3022847175598145 Accuracy: 0.1015625\n",
            "Loss: 2.300503730773926 Accuracy: 0.09375\n",
            "Loss: 2.298471450805664 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.313941240310669 Accuracy: 0.046875\n",
            "Loss: 2.3005897998809814 Accuracy: 0.078125\n",
            "Loss: 2.2988944053649902 Accuracy: 0.140625\n",
            "Loss: 2.300020933151245 Accuracy: 0.125\n",
            "Loss: 2.306791067123413 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2936387062072754 Accuracy: 0.171875\n",
            "Loss: 2.29866886138916 Accuracy: 0.1484375\n",
            "Loss: 2.304110288619995 Accuracy: 0.09375\n",
            "Loss: 2.304154872894287 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.298123598098755 Accuracy: 0.1171875\n",
            "Loss: 2.3059372901916504 Accuracy: 0.1171875\n",
            "Loss: 2.2979519367218018 Accuracy: 0.1484375\n",
            "Loss: 2.3004913330078125 Accuracy: 0.1328125\n",
            "Loss: 2.2990407943725586 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2945597171783447 Accuracy: 0.1171875\n",
            "Loss: 2.3029518127441406 Accuracy: 0.078125\n",
            "Loss: 2.3019540309906006 Accuracy: 0.125\n",
            "Loss: 2.302814245223999 Accuracy: 0.109375\n",
            "Loss: 2.297447443008423 Accuracy: 0.140625\n",
            "Starting new epoch...\n",
            "Loss: 2.3035309314727783 Accuracy: 0.1015625\n",
            "Loss: 2.308479070663452 Accuracy: 0.09375\n",
            "Loss: 2.3047637939453125 Accuracy: 0.1015625\n",
            "Loss: 2.3100781440734863 Accuracy: 0.046875\n",
            "Starting new epoch...\n",
            "Loss: 2.303837776184082 Accuracy: 0.1145833358168602\n",
            "Loss: 2.302469253540039 Accuracy: 0.109375\n",
            "Loss: 2.3026928901672363 Accuracy: 0.0859375\n",
            "Loss: 2.3013176918029785 Accuracy: 0.125\n",
            "Loss: 2.3005623817443848 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.3109025955200195 Accuracy: 0.0859375\n",
            "Loss: 2.299990653991699 Accuracy: 0.09375\n",
            "Loss: 2.305464506149292 Accuracy: 0.078125\n",
            "Loss: 2.290703296661377 Accuracy: 0.1484375\n",
            "Loss: 2.2989842891693115 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.2981793880462646 Accuracy: 0.1171875\n",
            "Loss: 2.2949657440185547 Accuracy: 0.1484375\n",
            "Loss: 2.296647071838379 Accuracy: 0.1328125\n",
            "Loss: 2.299886465072632 Accuracy: 0.1328125\n",
            "Loss: 2.3030500411987305 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.307274341583252 Accuracy: 0.1015625\n",
            "Loss: 2.303140163421631 Accuracy: 0.109375\n",
            "Loss: 2.3041880130767822 Accuracy: 0.1015625\n",
            "Loss: 2.3052384853363037 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2976465225219727 Accuracy: 0.1328125\n",
            "Loss: 2.299085855484009 Accuracy: 0.1328125\n",
            "Loss: 2.300321102142334 Accuracy: 0.1171875\n",
            "Loss: 2.3030643463134766 Accuracy: 0.0859375\n",
            "Loss: 2.2971415519714355 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.2945432662963867 Accuracy: 0.15625\n",
            "Loss: 2.3068933486938477 Accuracy: 0.1015625\n",
            "Loss: 2.3000056743621826 Accuracy: 0.109375\n",
            "Loss: 2.301745653152466 Accuracy: 0.1015625\n",
            "Loss: 2.3043336868286133 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.302065372467041 Accuracy: 0.0703125\n",
            "Loss: 2.2959117889404297 Accuracy: 0.109375\n",
            "Loss: 2.3069896697998047 Accuracy: 0.078125\n",
            "Loss: 2.302946090698242 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.30822491645813 Accuracy: 0.0859375\n",
            "Loss: 2.3030145168304443 Accuracy: 0.125\n",
            "Loss: 2.3002147674560547 Accuracy: 0.1328125\n",
            "Loss: 2.294729471206665 Accuracy: 0.125\n",
            "Loss: 2.3016316890716553 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.303424835205078 Accuracy: 0.0859375\n",
            "Loss: 2.305830955505371 Accuracy: 0.09375\n",
            "Loss: 2.306076765060425 Accuracy: 0.0625\n",
            "Loss: 2.30267333984375 Accuracy: 0.0859375\n",
            "Loss: 2.291388988494873 Accuracy: 0.1484375\n",
            "Starting new epoch...\n",
            "Loss: 2.303229808807373 Accuracy: 0.1328125\n",
            "Loss: 2.303121566772461 Accuracy: 0.1328125\n",
            "Loss: 2.306257724761963 Accuracy: 0.1015625\n",
            "Loss: 2.3008203506469727 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.2952475547790527 Accuracy: 0.1171875\n",
            "Loss: 2.299774169921875 Accuracy: 0.125\n",
            "Loss: 2.314241647720337 Accuracy: 0.0703125\n",
            "Loss: 2.2972850799560547 Accuracy: 0.1328125\n",
            "Loss: 2.3093690872192383 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.306626796722412 Accuracy: 0.0625\n",
            "Loss: 2.3044445514678955 Accuracy: 0.078125\n",
            "Loss: 2.3005595207214355 Accuracy: 0.109375\n",
            "Loss: 2.3025941848754883 Accuracy: 0.1015625\n",
            "Loss: 2.30670166015625 Accuracy: 0.0703125\n",
            "Starting new epoch...\n",
            "Loss: 2.296492338180542 Accuracy: 0.1171875\n",
            "Loss: 2.3023152351379395 Accuracy: 0.1171875\n",
            "Loss: 2.3015854358673096 Accuracy: 0.09375\n",
            "Loss: 2.307544708251953 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.2986302375793457 Accuracy: 0.1640625\n",
            "Loss: 2.302448272705078 Accuracy: 0.125\n",
            "Loss: 2.3014636039733887 Accuracy: 0.1328125\n",
            "Loss: 2.3059072494506836 Accuracy: 0.1015625\n",
            "Loss: 2.3036036491394043 Accuracy: 0.078125\n",
            "Starting new epoch...\n",
            "Loss: 2.3033719062805176 Accuracy: 0.0859375\n",
            "Loss: 2.299049139022827 Accuracy: 0.1328125\n",
            "Loss: 2.2957420349121094 Accuracy: 0.140625\n",
            "Loss: 2.3052241802215576 Accuracy: 0.0859375\n",
            "Loss: 2.2991936206817627 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.2998666763305664 Accuracy: 0.1015625\n",
            "Loss: 2.3015520572662354 Accuracy: 0.109375\n",
            "Loss: 2.2996158599853516 Accuracy: 0.125\n",
            "Loss: 2.30232572555542 Accuracy: 0.078125\n",
            "Loss: 2.296471118927002 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2953360080718994 Accuracy: 0.1484375\n",
            "Loss: 2.299670696258545 Accuracy: 0.1171875\n",
            "Loss: 2.2987191677093506 Accuracy: 0.1328125\n",
            "Loss: 2.303300380706787 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2969119548797607 Accuracy: 0.140625\n",
            "Loss: 2.2969536781311035 Accuracy: 0.1171875\n",
            "Loss: 2.2927286624908447 Accuracy: 0.140625\n",
            "Loss: 2.303642511367798 Accuracy: 0.1015625\n",
            "Loss: 2.2977867126464844 Accuracy: 0.1484375\n",
            "Starting new epoch...\n",
            "Loss: 2.303473949432373 Accuracy: 0.109375\n",
            "Loss: 2.303013563156128 Accuracy: 0.0859375\n",
            "Loss: 2.3098502159118652 Accuracy: 0.0859375\n",
            "Loss: 2.3082480430603027 Accuracy: 0.0703125\n",
            "Loss: 2.3008158206939697 Accuracy: 0.1484375\n",
            "Starting new epoch...\n",
            "Loss: 2.298121929168701 Accuracy: 0.1171875\n",
            "Loss: 2.301239013671875 Accuracy: 0.109375\n",
            "Loss: 2.2978038787841797 Accuracy: 0.1328125\n",
            "Loss: 2.298988103866577 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.3054592609405518 Accuracy: 0.109375\n",
            "Loss: 2.3108811378479004 Accuracy: 0.078125\n",
            "Loss: 2.3058528900146484 Accuracy: 0.0859375\n",
            "Loss: 2.305480480194092 Accuracy: 0.1015625\n",
            "Loss: 2.29825496673584 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.3014678955078125 Accuracy: 0.078125\n",
            "Loss: 2.3104658126831055 Accuracy: 0.078125\n",
            "Loss: 2.310567855834961 Accuracy: 0.0703125\n",
            "Loss: 2.2937142848968506 Accuracy: 0.1875\n",
            "Loss: 2.295774459838867 Accuracy: 0.171875\n",
            "Starting new epoch...\n",
            "Loss: 2.2953648567199707 Accuracy: 0.15625\n",
            "Loss: 2.3024911880493164 Accuracy: 0.109375\n",
            "Loss: 2.294861316680908 Accuracy: 0.1328125\n",
            "Loss: 2.305973768234253 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.2976512908935547 Accuracy: 0.15625\n",
            "Loss: 2.2910139560699463 Accuracy: 0.1328125\n",
            "Loss: 2.293163537979126 Accuracy: 0.125\n",
            "Loss: 2.302250862121582 Accuracy: 0.1015625\n",
            "Loss: 2.297694206237793 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.3012847900390625 Accuracy: 0.109375\n",
            "Loss: 2.2942616939544678 Accuracy: 0.1953125\n",
            "Loss: 2.296318292617798 Accuracy: 0.125\n",
            "Loss: 2.295576572418213 Accuracy: 0.1484375\n",
            "Loss: 2.3068647384643555 Accuracy: 0.140625\n",
            "Starting new epoch...\n",
            "Loss: 2.298837900161743 Accuracy: 0.125\n",
            "Loss: 2.303832769393921 Accuracy: 0.1171875\n",
            "Loss: 2.301661968231201 Accuracy: 0.109375\n",
            "Loss: 2.3057899475097656 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2967045307159424 Accuracy: 0.109375\n",
            "Loss: 2.3052821159362793 Accuracy: 0.109375\n",
            "Loss: 2.3023321628570557 Accuracy: 0.1640625\n",
            "Loss: 2.301882266998291 Accuracy: 0.1171875\n",
            "Loss: 2.2953996658325195 Accuracy: 0.1328125\n",
            "Starting new epoch...\n",
            "Loss: 2.2918200492858887 Accuracy: 0.2109375\n",
            "Loss: 2.294158935546875 Accuracy: 0.1796875\n",
            "Loss: 2.302588701248169 Accuracy: 0.078125\n",
            "Loss: 2.3033652305603027 Accuracy: 0.09375\n",
            "Loss: 2.299722671508789 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.2951483726501465 Accuracy: 0.1328125\n",
            "Loss: 2.311396837234497 Accuracy: 0.0703125\n",
            "Loss: 2.304304838180542 Accuracy: 0.09375\n",
            "Loss: 2.2987823486328125 Accuracy: 0.125\n",
            "Loss: 2.2979958057403564 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.2943801879882812 Accuracy: 0.171875\n",
            "Loss: 2.2989866733551025 Accuracy: 0.125\n",
            "Loss: 2.2997918128967285 Accuracy: 0.125\n",
            "Loss: 2.296921730041504 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.2936031818389893 Accuracy: 0.1484375\n",
            "Loss: 2.297140121459961 Accuracy: 0.140625\n",
            "Loss: 2.3005757331848145 Accuracy: 0.09375\n",
            "Loss: 2.2976675033569336 Accuracy: 0.15625\n",
            "Loss: 2.2995550632476807 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.2882323265075684 Accuracy: 0.1484375\n",
            "Loss: 2.3056888580322266 Accuracy: 0.1328125\n",
            "Loss: 2.296175718307495 Accuracy: 0.109375\n",
            "Loss: 2.300166606903076 Accuracy: 0.1328125\n",
            "Loss: 2.2922463417053223 Accuracy: 0.1640625\n",
            "Starting new epoch...\n",
            "Loss: 2.300118923187256 Accuracy: 0.1015625\n",
            "Loss: 2.3028481006622314 Accuracy: 0.0859375\n",
            "Loss: 2.3077969551086426 Accuracy: 0.0390625\n",
            "Loss: 2.2978758811950684 Accuracy: 0.0859375\n",
            "Starting new epoch...\n",
            "Loss: 2.295294761657715 Accuracy: 0.15625\n",
            "Loss: 2.302652359008789 Accuracy: 0.1015625\n",
            "Loss: 2.2978298664093018 Accuracy: 0.1015625\n",
            "Loss: 2.303546905517578 Accuracy: 0.09375\n",
            "Loss: 2.3054869174957275 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.29964542388916 Accuracy: 0.140625\n",
            "Loss: 2.300436496734619 Accuracy: 0.09375\n",
            "Loss: 2.291430950164795 Accuracy: 0.1875\n",
            "Loss: 2.30456805229187 Accuracy: 0.1015625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCQGEopwD452",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07642d1e-1210-4d11-ee5a-b2670e715d17"
      },
      "source": [
        "pred_layer1 = tf.nn.relu(tf.matmul(data.test_data, W1) + b1) \n",
        "pred_layer2 = tf.nn.relu(tf.matmul(pred_layer1, W2) + b2) \n",
        "test_preds = tf.argmax(tf.nn.softmax(tf.matmul(pred_layer2, W3) + b3), axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "print(acc)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.1135, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6MimUkzMXiR",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Perceptron : single hidden layer model\n",
        "\n",
        "> 1 hidden layer with RELU activation function : 82% test accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odOMlGgQHToy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52caf911-6d1f-44d8-82ea-c651fef09a0a"
      },
      "source": [
        "#weights and biases for first hidden layer\n",
        "W4 = tf.Variable(tf.random.uniform([784, 64]), dtype=tf.float32) \n",
        "b4 = tf.Variable(tf.random.uniform([64]), dtype=tf.float32)\n",
        "#weights and biases for output layer\n",
        "W5 = tf.Variable(tf.random.uniform([64, 10]), dtype=tf.float32) \n",
        "b5 = tf.Variable(tf.random.uniform([10]), dtype=tf.float32)\n",
        "\n",
        "for x in range(epochs):\n",
        "    for step in range(train_steps):\n",
        "        img_batch, lbl_batch = data.next_batch()\n",
        "        with tf.GradientTape() as tape:\n",
        "            hidden_layer_1_input = tf.add(tf.matmul(img_batch, W4), b4) \n",
        "            hidden_layer_1_output = tf.nn.relu(hidden_layer_1_input) \n",
        "            logits = tf.add(tf.matmul(hidden_layer_1_output, W5), b5)\n",
        "            xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                logits=logits, labels=lbl_batch))\n",
        "\n",
        "        grads = tape.gradient(xent, [W4, b4, W5, b5])\n",
        "        W4.assign_sub(learning_rate * grads[0])\n",
        "        b4.assign_sub(learning_rate * grads[1])\n",
        "        W5.assign_sub(learning_rate * grads[2])\n",
        "        b5.assign_sub(learning_rate * grads[3])\n",
        "\n",
        "\n",
        "        if not step % 100:\n",
        "            preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "            acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
        "                                 tf.float32))\n",
        "            print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 3.3362832069396973 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.2628839015960693 Accuracy: 0.2265625\n",
            "Loss: 2.256209373474121 Accuracy: 0.203125\n",
            "Loss: 2.236797571182251 Accuracy: 0.234375\n",
            "Loss: 2.210777759552002 Accuracy: 0.234375\n",
            "Starting new epoch...\n",
            "Loss: 2.2180581092834473 Accuracy: 0.21875\n",
            "Loss: 2.2246978282928467 Accuracy: 0.203125\n",
            "Loss: 2.1166858673095703 Accuracy: 0.296875\n",
            "Loss: 2.1847410202026367 Accuracy: 0.21875\n",
            "Loss: 2.163236141204834 Accuracy: 0.21875\n",
            "Starting new epoch...\n",
            "Loss: 2.15906023979187 Accuracy: 0.1875\n",
            "Loss: 2.1460037231445312 Accuracy: 0.171875\n",
            "Loss: 2.1420512199401855 Accuracy: 0.15625\n",
            "Loss: 2.1435585021972656 Accuracy: 0.234375\n",
            "Loss: 2.089242935180664 Accuracy: 0.2734375\n",
            "Starting new epoch...\n",
            "Loss: 2.076997756958008 Accuracy: 0.296875\n",
            "Loss: 2.041128158569336 Accuracy: 0.3046875\n",
            "Loss: 2.0448944568634033 Accuracy: 0.2734375\n",
            "Loss: 2.1606674194335938 Accuracy: 0.1796875\n",
            "Starting new epoch...\n",
            "Loss: 2.1219677925109863 Accuracy: 0.203125\n",
            "Loss: 2.054865598678589 Accuracy: 0.203125\n",
            "Loss: 1.9450639486312866 Accuracy: 0.2734375\n",
            "Loss: 2.0278520584106445 Accuracy: 0.2890625\n",
            "Loss: 1.9961483478546143 Accuracy: 0.28125\n",
            "Starting new epoch...\n",
            "Loss: 2.066282272338867 Accuracy: 0.296875\n",
            "Loss: 2.1172289848327637 Accuracy: 0.234375\n",
            "Loss: 1.99678635597229 Accuracy: 0.3046875\n",
            "Loss: 1.9867440462112427 Accuracy: 0.2890625\n",
            "Loss: 1.9990602731704712 Accuracy: 0.2890625\n",
            "Starting new epoch...\n",
            "Loss: 2.05300235748291 Accuracy: 0.265625\n",
            "Loss: 2.095146656036377 Accuracy: 0.1796875\n",
            "Loss: 1.9697905778884888 Accuracy: 0.3046875\n",
            "Loss: 1.9613410234451294 Accuracy: 0.2890625\n",
            "Starting new epoch...\n",
            "Loss: 2.052513599395752 Accuracy: 0.25\n",
            "Loss: 2.091282844543457 Accuracy: 0.2109375\n",
            "Loss: 2.058159351348877 Accuracy: 0.328125\n",
            "Loss: 1.9592875242233276 Accuracy: 0.34375\n",
            "Loss: 1.88649582862854 Accuracy: 0.3046875\n",
            "Starting new epoch...\n",
            "Loss: 1.826149344444275 Accuracy: 0.3671875\n",
            "Loss: 1.9876139163970947 Accuracy: 0.28125\n",
            "Loss: 1.9816901683807373 Accuracy: 0.3046875\n",
            "Loss: 1.8562345504760742 Accuracy: 0.3515625\n",
            "Loss: 2.112978935241699 Accuracy: 0.2109375\n",
            "Starting new epoch...\n",
            "Loss: 1.956416368484497 Accuracy: 0.3046875\n",
            "Loss: 1.8763515949249268 Accuracy: 0.3203125\n",
            "Loss: 2.0267834663391113 Accuracy: 0.328125\n",
            "Loss: 1.8611373901367188 Accuracy: 0.3828125\n",
            "Loss: 1.9612706899642944 Accuracy: 0.2421875\n",
            "Starting new epoch...\n",
            "Loss: 2.0468528270721436 Accuracy: 0.203125\n",
            "Loss: 1.9567586183547974 Accuracy: 0.4140625\n",
            "Loss: 1.969982385635376 Accuracy: 0.3359375\n",
            "Loss: 1.9627044200897217 Accuracy: 0.3203125\n",
            "Starting new epoch...\n",
            "Loss: 1.935459852218628 Accuracy: 0.3359375\n",
            "Loss: 1.8780006170272827 Accuracy: 0.34375\n",
            "Loss: 1.9664933681488037 Accuracy: 0.2734375\n",
            "Loss: 1.9398034811019897 Accuracy: 0.3984375\n",
            "Loss: 1.9437862634658813 Accuracy: 0.3359375\n",
            "Starting new epoch...\n",
            "Loss: 1.884429931640625 Accuracy: 0.34375\n",
            "Loss: 1.8743627071380615 Accuracy: 0.421875\n",
            "Loss: 1.930116891860962 Accuracy: 0.3515625\n",
            "Loss: 2.022233009338379 Accuracy: 0.328125\n",
            "Loss: 1.8790292739868164 Accuracy: 0.34375\n",
            "Starting new epoch...\n",
            "Loss: 1.8797073364257812 Accuracy: 0.3515625\n",
            "Loss: 1.8245701789855957 Accuracy: 0.3671875\n",
            "Loss: 1.9432852268218994 Accuracy: 0.4609375\n",
            "Loss: 1.9689342975616455 Accuracy: 0.3203125\n",
            "Starting new epoch...\n",
            "Loss: 1.9309594631195068 Accuracy: 0.34375\n",
            "Loss: 1.8690425157546997 Accuracy: 0.3515625\n",
            "Loss: 1.8490703105926514 Accuracy: 0.359375\n",
            "Loss: 1.9105591773986816 Accuracy: 0.3828125\n",
            "Loss: 1.9951965808868408 Accuracy: 0.3671875\n",
            "Starting new epoch...\n",
            "Loss: 1.8582332134246826 Accuracy: 0.3671875\n",
            "Loss: 1.8808872699737549 Accuracy: 0.375\n",
            "Loss: 1.7995009422302246 Accuracy: 0.4765625\n",
            "Loss: 1.8848695755004883 Accuracy: 0.359375\n",
            "Loss: 1.8249595165252686 Accuracy: 0.421875\n",
            "Starting new epoch...\n",
            "Loss: 1.8296773433685303 Accuracy: 0.3984375\n",
            "Loss: 1.7725027799606323 Accuracy: 0.4609375\n",
            "Loss: 1.7383091449737549 Accuracy: 0.515625\n",
            "Loss: 1.8020116090774536 Accuracy: 0.3828125\n",
            "Starting new epoch...\n",
            "Loss: 1.7506983280181885 Accuracy: 0.4921875\n",
            "Loss: 1.784666895866394 Accuracy: 0.46875\n",
            "Loss: 1.8953063488006592 Accuracy: 0.40625\n",
            "Loss: 1.8648258447647095 Accuracy: 0.4453125\n",
            "Loss: 1.849007487297058 Accuracy: 0.4375\n",
            "Starting new epoch...\n",
            "Loss: 1.7126355171203613 Accuracy: 0.5546875\n",
            "Loss: 1.8142637014389038 Accuracy: 0.5234375\n",
            "Loss: 1.6923620700836182 Accuracy: 0.546875\n",
            "Loss: 1.7854286432266235 Accuracy: 0.4140625\n",
            "Loss: 1.8677654266357422 Accuracy: 0.515625\n",
            "Starting new epoch...\n",
            "Loss: 1.7365472316741943 Accuracy: 0.515625\n",
            "Loss: 1.76527738571167 Accuracy: 0.46875\n",
            "Loss: 1.7454289197921753 Accuracy: 0.484375\n",
            "Loss: 1.669001817703247 Accuracy: 0.515625\n",
            "Starting new epoch...\n",
            "Loss: 1.675113558769226 Accuracy: 0.5234375\n",
            "Loss: 1.659481406211853 Accuracy: 0.515625\n",
            "Loss: 1.733171820640564 Accuracy: 0.5546875\n",
            "Loss: 1.7838544845581055 Accuracy: 0.46875\n",
            "Loss: 1.706855058670044 Accuracy: 0.484375\n",
            "Starting new epoch...\n",
            "Loss: 1.693040132522583 Accuracy: 0.53125\n",
            "Loss: 1.6593272686004639 Accuracy: 0.4765625\n",
            "Loss: 1.7745041847229004 Accuracy: 0.546875\n",
            "Loss: 1.6743091344833374 Accuracy: 0.5546875\n",
            "Loss: 1.6973729133605957 Accuracy: 0.53125\n",
            "Starting new epoch...\n",
            "Loss: 1.7436048984527588 Accuracy: 0.515625\n",
            "Loss: 1.593972086906433 Accuracy: 0.59375\n",
            "Loss: 1.6068780422210693 Accuracy: 0.53125\n",
            "Loss: 1.5875438451766968 Accuracy: 0.578125\n",
            "Loss: 1.6330569982528687 Accuracy: 0.546875\n",
            "Starting new epoch...\n",
            "Loss: 1.571075439453125 Accuracy: 0.515625\n",
            "Loss: 1.6344479322433472 Accuracy: 0.421875\n",
            "Loss: 1.5629513263702393 Accuracy: 0.5703125\n",
            "Loss: 1.6001384258270264 Accuracy: 0.46875\n",
            "Starting new epoch...\n",
            "Loss: 1.5494126081466675 Accuracy: 0.5390625\n",
            "Loss: 1.5167001485824585 Accuracy: 0.609375\n",
            "Loss: 1.5324552059173584 Accuracy: 0.5703125\n",
            "Loss: 1.4949557781219482 Accuracy: 0.6796875\n",
            "Loss: 1.5359137058258057 Accuracy: 0.671875\n",
            "Starting new epoch...\n",
            "Loss: 1.5740385055541992 Accuracy: 0.640625\n",
            "Loss: 1.4772639274597168 Accuracy: 0.609375\n",
            "Loss: 1.476935625076294 Accuracy: 0.6484375\n",
            "Loss: 1.5725440979003906 Accuracy: 0.59375\n",
            "Loss: 1.4805868864059448 Accuracy: 0.640625\n",
            "Starting new epoch...\n",
            "Loss: 1.5603152513504028 Accuracy: 0.5078125\n",
            "Loss: 1.4684302806854248 Accuracy: 0.65625\n",
            "Loss: 1.506781816482544 Accuracy: 0.4921875\n",
            "Loss: 1.414321780204773 Accuracy: 0.65625\n",
            "Starting new epoch...\n",
            "Loss: 1.4560539722442627 Accuracy: 0.6640625\n",
            "Loss: 1.454487681388855 Accuracy: 0.578125\n",
            "Loss: 1.3748385906219482 Accuracy: 0.59375\n",
            "Loss: 1.5308942794799805 Accuracy: 0.5703125\n",
            "Loss: 1.5213407278060913 Accuracy: 0.5390625\n",
            "Starting new epoch...\n",
            "Loss: 1.3880517482757568 Accuracy: 0.703125\n",
            "Loss: 1.4558615684509277 Accuracy: 0.625\n",
            "Loss: 1.3649730682373047 Accuracy: 0.65625\n",
            "Loss: 1.4052958488464355 Accuracy: 0.6328125\n",
            "Loss: 1.3531895875930786 Accuracy: 0.65625\n",
            "Starting new epoch...\n",
            "Loss: 1.3660696744918823 Accuracy: 0.6796875\n",
            "Loss: 1.2832465171813965 Accuracy: 0.6796875\n",
            "Loss: 1.3378574848175049 Accuracy: 0.671875\n",
            "Loss: 1.3578486442565918 Accuracy: 0.5546875\n",
            "Starting new epoch...\n",
            "Loss: 1.3170735836029053 Accuracy: 0.6640625\n",
            "Loss: 1.2684916257858276 Accuracy: 0.7109375\n",
            "Loss: 1.2546826601028442 Accuracy: 0.6796875\n",
            "Loss: 1.4907764196395874 Accuracy: 0.65625\n",
            "Loss: 1.329047441482544 Accuracy: 0.625\n",
            "Starting new epoch...\n",
            "Loss: 1.2698074579238892 Accuracy: 0.65625\n",
            "Loss: 1.262866497039795 Accuracy: 0.6796875\n",
            "Loss: 1.3140592575073242 Accuracy: 0.6484375\n",
            "Loss: 1.2916189432144165 Accuracy: 0.640625\n",
            "Loss: 1.469193458557129 Accuracy: 0.546875\n",
            "Starting new epoch...\n",
            "Loss: 1.200646996498108 Accuracy: 0.6796875\n",
            "Loss: 1.1962393522262573 Accuracy: 0.7109375\n",
            "Loss: 1.2724359035491943 Accuracy: 0.671875\n",
            "Loss: 1.264384388923645 Accuracy: 0.6875\n",
            "Starting new epoch...\n",
            "Loss: 1.2280272245407104 Accuracy: 0.65625\n",
            "Loss: 1.2518879175186157 Accuracy: 0.6953125\n",
            "Loss: 1.1753993034362793 Accuracy: 0.7109375\n",
            "Loss: 1.169893503189087 Accuracy: 0.7421875\n",
            "Loss: 1.0998693704605103 Accuracy: 0.7578125\n",
            "Starting new epoch...\n",
            "Loss: 1.0640368461608887 Accuracy: 0.7421875\n",
            "Loss: 1.170776605606079 Accuracy: 0.671875\n",
            "Loss: 1.318846583366394 Accuracy: 0.5\n",
            "Loss: 1.1266241073608398 Accuracy: 0.7421875\n",
            "Loss: 1.18904709815979 Accuracy: 0.703125\n",
            "Starting new epoch...\n",
            "Loss: 1.1979329586029053 Accuracy: 0.703125\n",
            "Loss: 1.0941625833511353 Accuracy: 0.7109375\n",
            "Loss: 1.0839333534240723 Accuracy: 0.78125\n",
            "Loss: 1.1759552955627441 Accuracy: 0.6640625\n",
            "Starting new epoch...\n",
            "Loss: 1.1246459484100342 Accuracy: 0.6875\n",
            "Loss: 1.1569678783416748 Accuracy: 0.6640625\n",
            "Loss: 1.175723671913147 Accuracy: 0.671875\n",
            "Loss: 1.1445627212524414 Accuracy: 0.625\n",
            "Loss: 1.0345667600631714 Accuracy: 0.7421875\n",
            "Starting new epoch...\n",
            "Loss: 1.102744698524475 Accuracy: 0.7109375\n",
            "Loss: 1.0307568311691284 Accuracy: 0.7578125\n",
            "Loss: 1.05534827709198 Accuracy: 0.6953125\n",
            "Loss: 1.06234872341156 Accuracy: 0.71875\n",
            "Loss: 1.2356750965118408 Accuracy: 0.6171875\n",
            "Starting new epoch...\n",
            "Loss: 1.2289609909057617 Accuracy: 0.6015625\n",
            "Loss: 1.1282737255096436 Accuracy: 0.7421875\n",
            "Loss: 1.061255693435669 Accuracy: 0.7265625\n",
            "Loss: 0.99498450756073 Accuracy: 0.78125\n",
            "Loss: 1.055729866027832 Accuracy: 0.6328125\n",
            "Starting new epoch...\n",
            "Loss: 0.9320470094680786 Accuracy: 0.7890625\n",
            "Loss: 1.0454907417297363 Accuracy: 0.7578125\n",
            "Loss: 1.0617351531982422 Accuracy: 0.6953125\n",
            "Loss: 1.1432822942733765 Accuracy: 0.6796875\n",
            "Starting new epoch...\n",
            "Loss: 1.101096749305725 Accuracy: 0.703125\n",
            "Loss: 0.9444472789764404 Accuracy: 0.796875\n",
            "Loss: 1.0750155448913574 Accuracy: 0.640625\n",
            "Loss: 1.0470061302185059 Accuracy: 0.671875\n",
            "Loss: 1.0479118824005127 Accuracy: 0.703125\n",
            "Starting new epoch...\n",
            "Loss: 0.9706013202667236 Accuracy: 0.7265625\n",
            "Loss: 0.8828059434890747 Accuracy: 0.8046875\n",
            "Loss: 1.0095361471176147 Accuracy: 0.71875\n",
            "Loss: 1.0128765106201172 Accuracy: 0.703125\n",
            "Loss: 0.935938835144043 Accuracy: 0.8203125\n",
            "Starting new epoch...\n",
            "Loss: 1.062842845916748 Accuracy: 0.75\n",
            "Loss: 0.9203937649726868 Accuracy: 0.8046875\n",
            "Loss: 1.0490059852600098 Accuracy: 0.7109375\n",
            "Loss: 0.9889997839927673 Accuracy: 0.7734375\n",
            "Starting new epoch...\n",
            "Loss: 1.0098696947097778 Accuracy: 0.7421875\n",
            "Loss: 1.1966497898101807 Accuracy: 0.6328125\n",
            "Loss: 0.9343782663345337 Accuracy: 0.7734375\n",
            "Loss: 0.9745835661888123 Accuracy: 0.734375\n",
            "Loss: 0.9623370170593262 Accuracy: 0.75\n",
            "Starting new epoch...\n",
            "Loss: 0.9377460479736328 Accuracy: 0.7109375\n",
            "Loss: 0.9529136419296265 Accuracy: 0.7734375\n",
            "Loss: 0.9949571490287781 Accuracy: 0.71875\n",
            "Loss: 0.9460386037826538 Accuracy: 0.7421875\n",
            "Loss: 0.8848364949226379 Accuracy: 0.765625\n",
            "Starting new epoch...\n",
            "Loss: 0.9223103523254395 Accuracy: 0.7890625\n",
            "Loss: 0.8099599480628967 Accuracy: 0.8359375\n",
            "Loss: 0.9253478646278381 Accuracy: 0.7265625\n",
            "Loss: 0.9015049934387207 Accuracy: 0.8203125\n",
            "Starting new epoch...\n",
            "Loss: 0.7619895339012146 Accuracy: 0.796875\n",
            "Loss: 1.0394495725631714 Accuracy: 0.6796875\n",
            "Loss: 0.9226545691490173 Accuracy: 0.7109375\n",
            "Loss: 1.0364168882369995 Accuracy: 0.7109375\n",
            "Loss: 0.8604965209960938 Accuracy: 0.78125\n",
            "Starting new epoch...\n",
            "Loss: 0.9396132230758667 Accuracy: 0.6640625\n",
            "Loss: 0.9395612478256226 Accuracy: 0.703125\n",
            "Loss: 0.9057261943817139 Accuracy: 0.765625\n",
            "Loss: 0.9937090873718262 Accuracy: 0.5859375\n",
            "Loss: 0.8407837152481079 Accuracy: 0.765625\n",
            "Starting new epoch...\n",
            "Loss: 0.8471893072128296 Accuracy: 0.8046875\n",
            "Loss: 0.9011368155479431 Accuracy: 0.7421875\n",
            "Loss: 0.8254920244216919 Accuracy: 0.796875\n",
            "Loss: 0.7987359762191772 Accuracy: 0.7421875\n",
            "Starting new epoch...\n",
            "Loss: 1.020693302154541 Accuracy: 0.6796875\n",
            "Loss: 0.7870678901672363 Accuracy: 0.8046875\n",
            "Loss: 0.8309895992279053 Accuracy: 0.8125\n",
            "Loss: 0.8636579513549805 Accuracy: 0.734375\n",
            "Loss: 0.7633298635482788 Accuracy: 0.8359375\n",
            "Starting new epoch...\n",
            "Loss: 0.9235608577728271 Accuracy: 0.6796875\n",
            "Loss: 0.7672989964485168 Accuracy: 0.7890625\n",
            "Loss: 0.9659199118614197 Accuracy: 0.6875\n",
            "Loss: 0.8510698080062866 Accuracy: 0.7578125\n",
            "Loss: 0.7092511653900146 Accuracy: 0.8203125\n",
            "Starting new epoch...\n",
            "Loss: 0.7083367109298706 Accuracy: 0.828125\n",
            "Loss: 1.053278923034668 Accuracy: 0.6875\n",
            "Loss: 0.812358021736145 Accuracy: 0.828125\n",
            "Loss: 0.8709298968315125 Accuracy: 0.7109375\n",
            "Loss: 1.0480306148529053 Accuracy: 0.6015625\n",
            "Starting new epoch...\n",
            "Loss: 0.8501627445220947 Accuracy: 0.7421875\n",
            "Loss: 0.7641756534576416 Accuracy: 0.78125\n",
            "Loss: 0.920001208782196 Accuracy: 0.6875\n",
            "Loss: 0.921442985534668 Accuracy: 0.75\n",
            "Starting new epoch...\n",
            "Loss: 0.8455251455307007 Accuracy: 0.8046875\n",
            "Loss: 0.7852948904037476 Accuracy: 0.7890625\n",
            "Loss: 0.7368792295455933 Accuracy: 0.8125\n",
            "Loss: 0.8061098456382751 Accuracy: 0.8046875\n",
            "Loss: 0.7974317073822021 Accuracy: 0.6875\n",
            "Starting new epoch...\n",
            "Loss: 0.6565513014793396 Accuracy: 0.84375\n",
            "Loss: 0.7769798636436462 Accuracy: 0.7421875\n",
            "Loss: 0.8329770565032959 Accuracy: 0.7578125\n",
            "Loss: 0.8219977021217346 Accuracy: 0.7109375\n",
            "Loss: 0.6501275300979614 Accuracy: 0.828125\n",
            "Starting new epoch...\n",
            "Loss: 0.7974950075149536 Accuracy: 0.7734375\n",
            "Loss: 0.804540753364563 Accuracy: 0.7734375\n",
            "Loss: 0.9249618053436279 Accuracy: 0.75\n",
            "Loss: 0.8291865587234497 Accuracy: 0.734375\n",
            "Starting new epoch...\n",
            "Loss: 0.8274027705192566 Accuracy: 0.75\n",
            "Loss: 0.8186249136924744 Accuracy: 0.734375\n",
            "Loss: 0.9327165484428406 Accuracy: 0.7109375\n",
            "Loss: 0.9168329238891602 Accuracy: 0.703125\n",
            "Loss: 0.7999697327613831 Accuracy: 0.734375\n",
            "Starting new epoch...\n",
            "Loss: 0.7645250558853149 Accuracy: 0.734375\n",
            "Loss: 0.7858285307884216 Accuracy: 0.8125\n",
            "Loss: 0.834679365158081 Accuracy: 0.7578125\n",
            "Loss: 0.8368879556655884 Accuracy: 0.765625\n",
            "Loss: 0.7418670654296875 Accuracy: 0.8125\n",
            "Starting new epoch...\n",
            "Loss: 0.7908924221992493 Accuracy: 0.7734375\n",
            "Loss: 0.8228066563606262 Accuracy: 0.765625\n",
            "Loss: 0.7278626561164856 Accuracy: 0.796875\n",
            "Loss: 0.7131774425506592 Accuracy: 0.78125\n",
            "Starting new epoch...\n",
            "Loss: 1.0266355276107788 Accuracy: 0.6171875\n",
            "Loss: 0.7688866257667542 Accuracy: 0.7734375\n",
            "Loss: 0.7497314214706421 Accuracy: 0.8046875\n",
            "Loss: 0.745354413986206 Accuracy: 0.78125\n",
            "Loss: 0.6840416193008423 Accuracy: 0.84375\n",
            "Starting new epoch...\n",
            "Loss: 0.6678664684295654 Accuracy: 0.796875\n",
            "Loss: 0.6772845387458801 Accuracy: 0.828125\n",
            "Loss: 0.7291214466094971 Accuracy: 0.8125\n",
            "Loss: 0.7950308322906494 Accuracy: 0.75\n",
            "Loss: 0.5289104580879211 Accuracy: 0.890625\n",
            "Starting new epoch...\n",
            "Loss: 0.7556953430175781 Accuracy: 0.7890625\n",
            "Loss: 0.8703979849815369 Accuracy: 0.65625\n",
            "Loss: 0.7481824159622192 Accuracy: 0.75\n",
            "Loss: 0.678713321685791 Accuracy: 0.828125\n",
            "Starting new epoch...\n",
            "Loss: 0.7894017696380615 Accuracy: 0.7421875\n",
            "Loss: 0.6926392912864685 Accuracy: 0.765625\n",
            "Loss: 0.7016310691833496 Accuracy: 0.8125\n",
            "Loss: 0.7790040969848633 Accuracy: 0.796875\n",
            "Loss: 0.6141901016235352 Accuracy: 0.8515625\n",
            "Starting new epoch...\n",
            "Loss: 0.6696906685829163 Accuracy: 0.8203125\n",
            "Loss: 0.754205584526062 Accuracy: 0.796875\n",
            "Loss: 0.6583495140075684 Accuracy: 0.8671875\n",
            "Loss: 0.6523259878158569 Accuracy: 0.78125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9e3tJwWM_xp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "178f46e9-7e41-4049-b1c8-a4c582dbad0a"
      },
      "source": [
        "print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.6618341207504272 Accuracy: 0.78125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTDV3PQWOtna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6062a5b-dfdc-4ca1-9922-c62a3b4333a7"
      },
      "source": [
        "logits_model2 = tf.nn.relu(tf.matmul(data.test_data, W4) + b4) \n",
        "test_preds = tf.argmax(tf.add(tf.matmul(logits_model2, W5) , b5), axis=1, output_type=tf.int32)\n",
        "\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "print(acc)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.8242, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63BHww8PY9za",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> The accuracy of MLP with 2 hidden layers and sigmoid activation function at the last layer decreases drastically to 11% in comparison to the actual linear model provided as part of exercise solution .\n",
        "\n",
        "> The accuracy of MLP with 1 hidden layer  is 72.6% which is better than a more  complex model.\n",
        "\n",
        "\n",
        "> Custom functions accepting paramters for layer dimensions, epochs, learning rate and activation functions  created. For this, I am using keras as it simplifies the development of models by accepting different parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVt_lN7WLAif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#Network parameters\n",
        "input_layer = 784    #24x24 images flattened\n",
        "output_layer = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "class Network:  \n",
        "  def update_weights(self, index, gradient):\n",
        "        self.weights[index].assign_sub(self.learning_rate * gradient)\n",
        "        \n",
        "  def update_bias(self, index, gradient):\n",
        "        self.bias[index].assign_sub(self.learning_rate * gradient)        \n",
        "        \n",
        "  def __init__(self, dimensions, learning_rate, activation_function):\n",
        "       # \"dimensions\" : dimension of the network. (input, hidden layers, output)\n",
        "       # \"activations\" : for activation funtion \n",
        "      \n",
        "    self.dimen = dimensions\n",
        "    self.no_of_layers = len(dimensions)\n",
        "    self.loss = None\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    # Weights and bias \n",
        "    self.weights = {}\n",
        "    self.bias = {}\n",
        "    self.activation = activation_function\n",
        "\n",
        "    for i in range(len(dimensions) - 1):\n",
        "      self.bias[i + 1] = tf.Variable(np.zeros(dimensions[i + 1], dtype=np.float32))# bias initialised to zero\n",
        "      self.weights[i + 1] = tf.Variable(tf.random.normal([dimensions[i], dimensions[i + 1]], 0, .1)) #weights initialised to random normal between -1 and 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUnwe0OQLM7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, train_steps): \n",
        "  for step in range(train_steps):\n",
        "      img_batch, lbl_batch = data.next_batch()\n",
        "      parameters =[]\n",
        "      with tf.GradientTape() as tape:\n",
        "          hidden_layer = img_batch #input layer\n",
        "          for layer in range(len(net.dimen) - 2): #2 for input and output layer\n",
        "              if(net.activation == \"tanh\"):\n",
        "                hidden_layer = tf.nn.tanh(tf.matmul(hidden_layer, net.weights[layer+1]) + net.bias[layer+1])\n",
        "              else:\n",
        "                hidden_layer = tf.nn.relu(tf.matmul(hidden_layer, net.weights[layer+1]) + net.bias[layer+1])\n",
        "              parameters.append(net.weights[layer+1]) \n",
        "              parameters.append(net.bias[layer+1])\n",
        "   \n",
        "          last_layer_index = len(net.dimen)-1 # output layer\n",
        "          parameters.append(net.weights[last_layer_index])\n",
        "          parameters.append(net.bias[last_layer_index]) # 10 mnist labels\n",
        "          logits = tf.matmul(hidden_layer, net.weights[last_layer_index]) + net.bias[last_layer_index]\n",
        "          xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=lbl_batch))\n",
        "         \n",
        "      grads = tape.gradient(xent, parameters)\n",
        "      for param_index in range(1,len(parameters),2):\n",
        "        net.update_weights(param_index//2 + 1,grads[param_index-1]) #first param for W\n",
        "        net.update_bias(param_index//2 + 1,grads[param_index]) #second param for b\n",
        "\n",
        "        if not step % 100:\n",
        "          preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "          acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),tf.float32))\n",
        "          print(\"epoch:{} / {}\".format(step,train_steps)+\" Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XJGgbtfsG1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net):\n",
        "    hidden_layer = data.test_data \n",
        "    for layer in range(len(net.dimen) - 2): \n",
        "      hidden_layer = tf.nn.relu(tf.matmul(hidden_layer, net.weights[layer+1]) + net.bias[layer+1])\n",
        "           \n",
        "    last_layer_index = len(net.dimen)-1 # output layer\n",
        "    logits = tf.nn.sigmoid(tf.matmul(hidden_layer, net.weights[last_layer_index]) + net.bias[last_layer_index])\n",
        "    test_preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "\n",
        "    acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),tf.float32))\n",
        "    print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RugSdXlo5kII",
        "colab_type": "text"
      },
      "source": [
        "Single- hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieTP6xpG5d66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "6a5ba98f-df82-44fc-e9d9-4b84a0b89958"
      },
      "source": [
        "learning_rate = 0.1\n",
        "net = Network((784,256, 10), learning_rate, \"relu\")\n",
        "train(net, 1000)\n",
        "test(net)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 / 1000 Loss: 2.3026344776153564 Accuracy: 0.1328125\n",
            "epoch:0 / 1000 Loss: 2.3026344776153564 Accuracy: 0.1328125\n",
            "epoch:100 / 1000 Loss: 2.302067279815674 Accuracy: 0.0859375\n",
            "epoch:100 / 1000 Loss: 2.302067279815674 Accuracy: 0.0859375\n",
            "epoch:200 / 1000 Loss: 2.29679274559021 Accuracy: 0.1171875\n",
            "epoch:200 / 1000 Loss: 2.29679274559021 Accuracy: 0.1171875\n",
            "epoch:300 / 1000 Loss: 2.295921564102173 Accuracy: 0.1171875\n",
            "epoch:300 / 1000 Loss: 2.295921564102173 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "epoch:400 / 1000 Loss: 2.2940824031829834 Accuracy: 0.1328125\n",
            "epoch:400 / 1000 Loss: 2.2940824031829834 Accuracy: 0.1328125\n",
            "epoch:500 / 1000 Loss: 2.2877655029296875 Accuracy: 0.0859375\n",
            "epoch:500 / 1000 Loss: 2.2877655029296875 Accuracy: 0.0859375\n",
            "epoch:600 / 1000 Loss: 2.2941079139709473 Accuracy: 0.1015625\n",
            "epoch:600 / 1000 Loss: 2.2941079139709473 Accuracy: 0.1015625\n",
            "epoch:700 / 1000 Loss: 2.289069414138794 Accuracy: 0.125\n",
            "epoch:700 / 1000 Loss: 2.289069414138794 Accuracy: 0.125\n",
            "epoch:800 / 1000 Loss: 2.2899527549743652 Accuracy: 0.140625\n",
            "epoch:800 / 1000 Loss: 2.2899527549743652 Accuracy: 0.140625\n",
            "Starting new epoch...\n",
            "epoch:900 / 1000 Loss: 2.2991786003112793 Accuracy: 0.1015625\n",
            "epoch:900 / 1000 Loss: 2.2991786003112793 Accuracy: 0.1015625\n",
            "Test accuracy: 0.10360000282526016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNl95jk4-n21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "1c7bfa16-74cb-45e0-a4a0-483c72a58bef"
      },
      "source": [
        "learning_rate = 0.1\n",
        "net = Network((784,256,128, 10), learning_rate, \"relu\")\n",
        "train(net, 1000)\n",
        "test(net)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 / 1000 Loss: 2.3022518157958984 Accuracy: 0.15625\n",
            "epoch:0 / 1000 Loss: 2.3022518157958984 Accuracy: 0.15625\n",
            "epoch:0 / 1000 Loss: 2.3022518157958984 Accuracy: 0.15625\n",
            "epoch:100 / 1000 Loss: 2.2936315536499023 Accuracy: 0.125\n",
            "epoch:100 / 1000 Loss: 2.2936315536499023 Accuracy: 0.125\n",
            "epoch:100 / 1000 Loss: 2.2936315536499023 Accuracy: 0.125\n",
            "epoch:200 / 1000 Loss: 2.2980496883392334 Accuracy: 0.0625\n",
            "epoch:200 / 1000 Loss: 2.2980496883392334 Accuracy: 0.0625\n",
            "epoch:200 / 1000 Loss: 2.2980496883392334 Accuracy: 0.0625\n",
            "epoch:300 / 1000 Loss: 2.296198606491089 Accuracy: 0.1640625\n",
            "epoch:300 / 1000 Loss: 2.296198606491089 Accuracy: 0.1640625\n",
            "epoch:300 / 1000 Loss: 2.296198606491089 Accuracy: 0.1640625\n",
            "Starting new epoch...\n",
            "epoch:400 / 1000 Loss: 2.2984964847564697 Accuracy: 0.1171875\n",
            "epoch:400 / 1000 Loss: 2.2984964847564697 Accuracy: 0.1171875\n",
            "epoch:400 / 1000 Loss: 2.2984964847564697 Accuracy: 0.1171875\n",
            "epoch:500 / 1000 Loss: 2.282299518585205 Accuracy: 0.15625\n",
            "epoch:500 / 1000 Loss: 2.282299518585205 Accuracy: 0.15625\n",
            "epoch:500 / 1000 Loss: 2.282299518585205 Accuracy: 0.15625\n",
            "epoch:600 / 1000 Loss: 2.29559588432312 Accuracy: 0.1328125\n",
            "epoch:600 / 1000 Loss: 2.29559588432312 Accuracy: 0.1328125\n",
            "epoch:600 / 1000 Loss: 2.29559588432312 Accuracy: 0.1328125\n",
            "epoch:700 / 1000 Loss: 2.2946128845214844 Accuracy: 0.0703125\n",
            "epoch:700 / 1000 Loss: 2.2946128845214844 Accuracy: 0.0703125\n",
            "epoch:700 / 1000 Loss: 2.2946128845214844 Accuracy: 0.0703125\n",
            "Starting new epoch...\n",
            "epoch:800 / 1000 Loss: 2.28916072845459 Accuracy: 0.1171875\n",
            "epoch:800 / 1000 Loss: 2.28916072845459 Accuracy: 0.1171875\n",
            "epoch:800 / 1000 Loss: 2.28916072845459 Accuracy: 0.1171875\n",
            "epoch:900 / 1000 Loss: 2.300276517868042 Accuracy: 0.078125\n",
            "epoch:900 / 1000 Loss: 2.300276517868042 Accuracy: 0.078125\n",
            "epoch:900 / 1000 Loss: 2.300276517868042 Accuracy: 0.078125\n",
            "Test accuracy: 0.1826000064611435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIsEdO0Mw5So",
        "colab_type": "text"
      },
      "source": [
        "MLP with 1 layer , relu activation function and Mean Squared error as loss function : 11.9% test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzZROYMOtoyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "efc078b4-6a4b-4195-a827-3a8866b6c31f"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3055 - accuracy: 0.1282\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3051 - accuracy: 0.1310\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3050 - accuracy: 0.1261\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3049 - accuracy: 0.1231\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3049 - accuracy: 0.1224\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1221\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1225\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1225\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1220\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f446fbdcb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob_ZixIevGZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9875a131-90fa-4ec0-bb31-41a3c35e48db"
      },
      "source": [
        "model.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 27.2433 - accuracy: 0.1193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27.243349075317383, 0.1193000003695488]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG-walxfxTCl",
        "colab_type": "text"
      },
      "source": [
        "MLP with 2 layers , relu activation function and Mean Squared error as loss function : 11.5% test accuracy\n",
        "\n",
        "> Takeaway : Model performance does not improves , so next I will try using different loss function to optimize \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-k-5NKDxPvj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1ccb7fc9-9e3f-4798-a537-d8341362526e"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3050 - accuracy: 0.1138\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1112\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1104\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1112\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3046 - accuracy: 0.1124\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3046 - accuracy: 0.1138\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3046 - accuracy: 0.1140\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3046 - accuracy: 0.1141\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1145\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f446d081ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Q1_jF4xRxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fb700442-a742-41c9-b447-c6033706370a"
      },
      "source": [
        "model.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 2ms/step - loss: 27.2432 - accuracy: 0.1150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27.243179321289062, 0.11500000208616257]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jde8I-1dnypT",
        "colab_type": "text"
      },
      "source": [
        "Keras MLP implementation with relu and softmax activation function and Sparse Categorical Cross Entropy : 97.7 % test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n73ayEqamtsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxEnS3pPmzk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5b044bcb-0efc-42c6-c283-cb814de4b4d0"
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5685 - accuracy: 0.9088\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5152 - accuracy: 0.9511\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5013 - accuracy: 0.9636\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4938 - accuracy: 0.9702\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4883 - accuracy: 0.9753\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4849 - accuracy: 0.9787\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4819 - accuracy: 0.9812\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4793 - accuracy: 0.9836\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4778 - accuracy: 0.9846\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4758 - accuracy: 0.9868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4479ce2400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePJwzRVQm2MX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "87b8c6b2-8d56-4e89-c5cc-fd9b3332f0f0"
      },
      "source": [
        "model.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4855 - accuracy: 0.9768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4854873418807983, 0.9768000245094299]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Thzhv8roQgv",
        "colab_type": "text"
      },
      "source": [
        "Keras MLP with 2 hidden layers using relu activation function and Sparse Categorical Cross Entropy as the loss function: 97.9% test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQSGMZu2oN2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8f2446c7-5629-4dcd-d92d-07c01e1adbfc"
      },
      "source": [
        "model1 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='sigmoid')\n",
        "])\n",
        "\n",
        "model1.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model1.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5477 - accuracy: 0.9173\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4962 - accuracy: 0.9600\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4868 - accuracy: 0.9702\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4817 - accuracy: 0.9754\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.4787 - accuracy: 0.9790\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4761 - accuracy: 0.9828\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4747 - accuracy: 0.9839\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4734 - accuracy: 0.9854\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4719 - accuracy: 0.9872\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4718 - accuracy: 0.9875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4477a3c630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A68sTq4tojQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ddf51d65-6094-4475-f3b4-72f6326688e1"
      },
      "source": [
        "model1.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4793 - accuracy: 0.9794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4792577028274536, 0.9793999791145325]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WGB74ncbVsp",
        "colab_type": "text"
      },
      "source": [
        "Multilayer Perceptron on Fashion MNIST\n",
        "\n",
        "\n",
        "> Use Sparse Categorical Cross Entropy loss function as it worked better for mnist dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ioBG5zTOw5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b8391bb1-262a-4b8c-d59c-cd6c1b3f3268"
      },
      "source": [
        "from datasets import MNISTDataset\n",
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "print(train_labels[0])\n",
        "plt.imshow(train_images[0], cmap=\"Greys_r\")\n",
        "\n",
        "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
        "                    test_images.reshape([-1, 784]), test_labels,\n",
        "                    batch_size=128)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YttPNqXwpHcv",
        "colab_type": "text"
      },
      "source": [
        "MLP Model with 1 Hidden layer and relu activation function : 86% test accuracy\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqgkkaZ7b0vj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8dd91fe4-2f46-4c54-8b09-5ad71d7a734c"
      },
      "source": [
        "fmnist_mlp_model_1 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_1.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_1.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6662 - accuracy: 0.8051\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6201 - accuracy: 0.8435\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6108 - accuracy: 0.8523\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6028 - accuracy: 0.8598\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5975 - accuracy: 0.8644\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5934 - accuracy: 0.8687\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5882 - accuracy: 0.8739\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5856 - accuracy: 0.8767\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5831 - accuracy: 0.8789\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5799 - accuracy: 0.8816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f447da3f400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwmKj28tlgB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "460e6858-712c-4c10-b4d6-d00d841c6a82"
      },
      "source": [
        "fmnist_mlp_model_1.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 2ms/step - loss: 1.5966 - accuracy: 0.8644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5966033935546875, 0.8644000291824341]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ann7lVDkpTzG",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 2 hidden layers with relu activation function : \n",
        "\n",
        "> Layer 1 : 128 units,relu\n",
        "\n",
        "\n",
        "> Layer 2 : 64 units,relu\n",
        "\n",
        "> 10 epochs : 86% test accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkX0e7AYl8ce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4a46c381-b7c5-4af0-8075-562451e4ffe9"
      },
      "source": [
        "fmnist_mlp_model_2 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_2.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_2.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7407 - accuracy: 0.7240\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6476 - accuracy: 0.8151\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6118 - accuracy: 0.8495\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6040 - accuracy: 0.8569\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5973 - accuracy: 0.8637\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5933 - accuracy: 0.8677\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5892 - accuracy: 0.8718\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5868 - accuracy: 0.8742\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5858 - accuracy: 0.8749\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5842 - accuracy: 0.8768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f445bc0b4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXtOEVLRp14M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac055ee2-eeda-4cbd-e3a8-32187337f5e1"
      },
      "source": [
        "fmnist_mlp_model_2.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.6009 - accuracy: 0.8596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6008858680725098, 0.8596000075340271]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wfwtaYmqZkR",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 1 layer and tanh activation function : 87% test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__5Lo6Rp_Fd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "02d88830-d36f-432f-e314-ce73208d3b53"
      },
      "source": [
        "fmnist_mlp_model_3 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='tanh'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_3.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_3.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.7138 - accuracy: 0.7547\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6693 - accuracy: 0.7940\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6259 - accuracy: 0.8388\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5963 - accuracy: 0.8690\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5901 - accuracy: 0.8738\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5831 - accuracy: 0.8821\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5804 - accuracy: 0.8842\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5763 - accuracy: 0.8878\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5728 - accuracy: 0.8911\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5709 - accuracy: 0.8931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f44740120f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGij9-FkqlMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fmnist_mlp_model_3.evaluate(test_images, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGslJ63S_Rl9",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 2 layers and relu activation function :\n",
        "\n",
        "> Layer 1 : 128 units, relu \n",
        "\n",
        "\n",
        "> Layer 2 : 64 units, relu\n",
        "\n",
        "> 10 epochs :  86% test accuracy\n",
        "\n",
        "> 30 epochs : 81% test accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69pI-Sdu-gKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "91d1d2c2-e789-499f-e07c-b5d82b74b48c"
      },
      "source": [
        "fmnist_mlp_model_4 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_4.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "fmnist_mlp_model_4.fit(train_images, train_labels, epochs=30)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7461 - accuracy: 0.7183\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6763 - accuracy: 0.7850\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6694 - accuracy: 0.7911\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6645 - accuracy: 0.7962\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6576 - accuracy: 0.8035\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6590 - accuracy: 0.8014\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6525 - accuracy: 0.8084\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6540 - accuracy: 0.8066\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6515 - accuracy: 0.8092\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6501 - accuracy: 0.8104\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6472 - accuracy: 0.8135\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6486 - accuracy: 0.8122\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6476 - accuracy: 0.8132\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6492 - accuracy: 0.8116\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6469 - accuracy: 0.8138\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6466 - accuracy: 0.8139\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6474 - accuracy: 0.8134\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6436 - accuracy: 0.8173\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6435 - accuracy: 0.8173\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6434 - accuracy: 0.8176\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6469 - accuracy: 0.8138\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6485 - accuracy: 0.8126\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6472 - accuracy: 0.8139\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6465 - accuracy: 0.8144\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6468 - accuracy: 0.8140\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6442 - accuracy: 0.8167\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6470 - accuracy: 0.8140\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6518 - accuracy: 0.8090\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6490 - accuracy: 0.8119\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6452 - accuracy: 0.8157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f44f080be80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NevQkw3j_190",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8c07171f-18df-4736-cc8d-d686271e7468"
      },
      "source": [
        "fmnist_mlp_model_4.evaluate(test_images, test_labels)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.6600 - accuracy: 0.8010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6600499153137207, 0.8009999990463257]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtj2j_3DqwQI",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 2 layers and tanh activation function :\n",
        "\n",
        "> Layer 1 : 128 units, tanh \n",
        "\n",
        "\n",
        "> Layer 2 : 64 units, tanh\n",
        "\n",
        "> 10 epochs :  86% test accuracy\n",
        "\n",
        "> 30 epochs : 86% test accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VMR3EEpqphI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc8be47e-69bf-452f-8b4d-a4fea5502c7a"
      },
      "source": [
        "fmnist_mlp_model_5 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='tanh'),\n",
        "    keras.layers.Dense(64, activation='tanh'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_5.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_5.fit(train_images, train_labels, epochs=30)\n",
        "\n",
        "fmnist_mlp_model_5.evaluate(test_images, test_labels)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6594 - accuracy: 0.8101\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6153 - accuracy: 0.8479\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6070 - accuracy: 0.8553\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5980 - accuracy: 0.8644\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5945 - accuracy: 0.8676\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5889 - accuracy: 0.8727\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5858 - accuracy: 0.8759\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5842 - accuracy: 0.8776\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5815 - accuracy: 0.8805\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5795 - accuracy: 0.8823\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5787 - accuracy: 0.8831\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5784 - accuracy: 0.8831\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5755 - accuracy: 0.8859\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5749 - accuracy: 0.8866\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5744 - accuracy: 0.8869\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5738 - accuracy: 0.8874\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5726 - accuracy: 0.8890\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5725 - accuracy: 0.8888\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5720 - accuracy: 0.8894\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5710 - accuracy: 0.8903\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5728 - accuracy: 0.8884\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5692 - accuracy: 0.8920\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5703 - accuracy: 0.8913\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5690 - accuracy: 0.8923\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5688 - accuracy: 0.8927\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5682 - accuracy: 0.8929\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5669 - accuracy: 0.8944\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5689 - accuracy: 0.8921\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5663 - accuracy: 0.8952\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5660 - accuracy: 0.8953\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.5942 - accuracy: 0.8672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5942496061325073, 0.8672000169754028]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnxgRrWUq9wU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "37bac900-bf82-4045-8d98-be3df842bf8b"
      },
      "source": [
        "fmnist_mlp_model_5.evaluate(test_images, test_labels)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.5942 - accuracy: 0.8672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5942496061325073, 0.8672000169754028]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UToP86Nrc4y",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 2 dense layers, one with relu and second layer with tanh activation : \n",
        "\n",
        "> Layer 1 : 128 units, relu\n",
        "\n",
        "> Layer 2 : 128 units, tanh\n",
        "\n",
        "> 10 epochs : 86.4% test accuracy\n",
        "\n",
        "> 30 epochs : 88% test accuracy\n",
        "\n",
        "> Takeaway: using different activation functions within the same model does not improve model performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw0HYBEArDrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "820f7108-daf4-4083-eb1f-10ef23bb2e6a"
      },
      "source": [
        "fmnist_mlp_model_6 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='tanh'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_6.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_6.fit(train_images, train_labels, epochs=30)\n",
        "\n",
        "fmnist_mlp_model_6.evaluate(test_images, test_labels)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6627 - accuracy: 0.8042\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6149 - accuracy: 0.8463\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6058 - accuracy: 0.8554\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5988 - accuracy: 0.8622\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5932 - accuracy: 0.8684\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5904 - accuracy: 0.8701\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5865 - accuracy: 0.8743\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5833 - accuracy: 0.8778\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5828 - accuracy: 0.8779\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5789 - accuracy: 0.8819\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5755 - accuracy: 0.8853\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5741 - accuracy: 0.8866\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5736 - accuracy: 0.8870\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5712 - accuracy: 0.8896\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5712 - accuracy: 0.8897\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5699 - accuracy: 0.8910\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5690 - accuracy: 0.8919\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5672 - accuracy: 0.8939\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5647 - accuracy: 0.8965\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5661 - accuracy: 0.8955\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5646 - accuracy: 0.8963\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5636 - accuracy: 0.8973\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5618 - accuracy: 0.8994\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5623 - accuracy: 0.8981\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5598 - accuracy: 0.9011\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5590 - accuracy: 0.9022\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5586 - accuracy: 0.9024\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5587 - accuracy: 0.9024\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5582 - accuracy: 0.9027\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5566 - accuracy: 0.9043\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.5795 - accuracy: 0.8814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5794687271118164, 0.8813999891281128]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy3t-SqF5F18",
        "colab_type": "text"
      },
      "source": [
        "Observations :\n",
        "\n",
        "> Random initialization of weights : \n",
        "\n",
        "> Epochs : Training the model for more epochs improved accuracy.\n",
        "\n",
        "> Activation Function : RELU activation function gave better accuracy for model as compared to tanh. Mixing activation function like relu and tanh within 2 layer MLP also improved the accuracy.\n",
        "\n",
        "> Loss Function : Changing the loss function from mean squared error to sparse cross entropy improved accuracy. MSE loss function does not work great for high dimensional data like images.\n",
        "\n",
        "> Change in layer dimensions for 2-layer model: 2-layer models in which layers dimension was reduced to half in the second layer didn't improve the accuracy even after executing for more epochs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfiVPeIj7lTf",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "\n",
        "1.   http://blog.ai.ovgu.de/posts/jens/2019/002_tf20_basic_mnist/index.html\n",
        "2.   https://www.tensorflow.org/tutorials/keras/classification\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zk50ZJrrcJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}