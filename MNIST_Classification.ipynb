{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOqZ7zpkg3ozHRnn2RLu8w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshudaur/TensorFlowProjects/blob/master/MNIST_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nAZAWm3s_8U",
        "colab_type": "text"
      },
      "source": [
        "MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H2wg3m15J50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f51f2cac-34f5-4022-8217-14f49cb9c16b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import MNISTDataset\n",
        "tf.__version__ "
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVIbLlPt61EK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "8de6f7bc-b22c-4bfd-bde1-4b614f4b0594"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "plt.imshow(train_images[0], cmap=\"Greys_r\")\n",
        "\n",
        "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
        "                    test_images.reshape([-1, 784]), test_labels,\n",
        "                    batch_size=128)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQTqmVD58DtG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48047346-aa97-49e7-b758-76db445c4855"
      },
      "source": [
        "train_steps = 1000\n",
        "learning_rate = 0.1\n",
        "# Outputs random values from a uniform distribution\n",
        "tf.random.uniform( shape=[],minval=-0.1,maxval=0.1, dtype=tf.dtypes.float32,seed=10)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.020330332>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_q9mk7ubgrH",
        "colab_type": "text"
      },
      "source": [
        "MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ets9zYCF9lWy",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "> 2 Hidden layers with RELU activation function : 11.35% test accuracy\n",
        "\n",
        "> Sigmoid activation function at the last layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZSMrTux9gGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1c66672d-8b9f-4ded-fc1e-352a6d987d87"
      },
      "source": [
        "\n",
        "#random initialization of weights and bias tensors(multi-dimensional arrays)\n",
        "#weights and biases for first hidden layer\n",
        "W1 = tf.Variable(np.zeros([784, 128]), dtype=tf.float32) \n",
        "b1 = tf.Variable(np.zeros([128]), dtype=tf.float32)\n",
        "#weights and biases for second hidden layer\n",
        "W2 = tf.Variable(np.zeros([128, 64]), dtype=tf.float32) \n",
        "b2 = tf.Variable(np.zeros([64]), dtype=tf.float32)\n",
        "#weights and biases for output layer\n",
        "W3 = tf.Variable(np.zeros([64, 10]), dtype=tf.float32) \n",
        "b3 = tf.Variable(np.zeros([10]), dtype=tf.float32)\n",
        "\n",
        "for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape() as tape:        \n",
        "        hidden_layer_1_input =  tf.add(tf.matmul(img_batch, W1), b1)\n",
        "        hidden_layer_1_output = tf.nn.relu(hidden_layer_1_input)\n",
        "        hidden_layer_2_input =  tf.add(tf.matmul(hidden_layer_1_output, W2),b2)\n",
        "        hidden_layer_2_output= tf.nn.relu(hidden_layer_2_input)\n",
        "        logits = tf.nn.sigmoid(tf.add(tf.matmul(hidden_layer_2_output, W3),b3))\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=lbl_batch))\n",
        "    #automatic differentiation    \n",
        "    grads = tape.gradient(xent, [W1, b1, W2, b2, W3, b3])\n",
        "    W1.assign_sub(learning_rate * grads[0])\n",
        "    b1.assign_sub(learning_rate * grads[1])\n",
        "    W2.assign_sub(learning_rate * grads[2])\n",
        "    b2.assign_sub(learning_rate * grads[3])\n",
        "    W3.assign_sub(learning_rate * grads[4])\n",
        "    b3.assign_sub(learning_rate * grads[5])\n",
        "  \n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
        "                            tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.3025851249694824 Accuracy: 0.1015625\n",
            "Loss: 2.302812099456787 Accuracy: 0.078125\n",
            "Loss: 2.3027336597442627 Accuracy: 0.125\n",
            "Loss: 2.3033456802368164 Accuracy: 0.0703125\n",
            "Loss: 2.301438570022583 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.301176071166992 Accuracy: 0.1484375\n",
            "Loss: 2.3023271560668945 Accuracy: 0.1171875\n",
            "Loss: 2.3022499084472656 Accuracy: 0.1171875\n",
            "Loss: 2.3000330924987793 Accuracy: 0.09375\n",
            "Loss: 2.301468849182129 Accuracy: 0.1015625\n",
            "Starting new epoch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCQGEopwD452",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b0714dc-e05d-40d6-e2f9-59acf0134d63"
      },
      "source": [
        "pred_layer1 = tf.nn.relu(tf.matmul(data.test_data, W1) + b1) \n",
        "pred_layer2 = tf.nn.relu(tf.matmul(pred_layer1, W2) + b2) \n",
        "test_preds = tf.argmax(tf.nn.softmax(tf.matmul(pred_layer2, W3) + b3), axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "print(acc)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.1135, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6MimUkzMXiR",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Perceptron : single hidden layer model\n",
        "\n",
        "> 1 hidden layer with RELU activation function : 82% test accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odOMlGgQHToy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "141200f3-1e3d-46f2-f039-6d789085cfdc"
      },
      "source": [
        "train_steps = 1000\n",
        "learning_rate = 0.1\n",
        "\n",
        "W = tf.Variable(np.zeros([784, 10]).astype(np.float32))\n",
        "b = tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "\n",
        "for step in range(train_steps):\n",
        "    img_batch, lbl_batch = data.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = tf.matmul(img_batch, W) + b\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=lbl_batch))\n",
        "        \n",
        "    grads = tape.gradient(xent, [W, b])\n",
        "    W.assign_sub(learning_rate * grads[0])\n",
        "    b.assign_sub(learning_rate * grads[1])\n",
        "    \n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),tf.float32))\n",
        "        print(\"epoch:{} / {}\".format(step,train_steps)+\"Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 / 1000Loss: 2.3025851249694824 Accuracy: 0.1171875\n",
            "epoch:100 / 1000Loss: 2.3004682064056396 Accuracy: 0.1015625\n",
            "epoch:200 / 1000Loss: 2.299160957336426 Accuracy: 0.1328125\n",
            "epoch:300 / 1000Loss: 2.296037197113037 Accuracy: 0.140625\n",
            "epoch:400 / 1000Loss: 2.3029394149780273 Accuracy: 0.0859375\n",
            "Starting new epoch...\n",
            "epoch:500 / 1000Loss: 2.2974483966827393 Accuracy: 0.1640625\n",
            "epoch:600 / 1000Loss: 2.2980539798736572 Accuracy: 0.125\n",
            "epoch:700 / 1000Loss: 2.3048200607299805 Accuracy: 0.0859375\n",
            "epoch:800 / 1000Loss: 2.2928032875061035 Accuracy: 0.140625\n",
            "Starting new epoch...\n",
            "epoch:900 / 1000Loss: 2.290800094604492 Accuracy: 0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9e3tJwWM_xp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4b7a3d0-4474-414d-fe4e-1fcb1af17ccc"
      },
      "source": [
        "print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.3023014068603516 Accuracy: 0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTDV3PQWOtna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6062a5b-dfdc-4ca1-9922-c62a3b4333a7"
      },
      "source": [
        "logits_model2 = tf.nn.relu(tf.matmul(data.test_data, W4) + b4) \n",
        "test_preds = tf.argmax(tf.add(tf.matmul(logits_model2, W5) , b5), axis=1, output_type=tf.int32)\n",
        "\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "print(acc)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.8242, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63BHww8PY9za",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> The accuracy of MLP with 2 hidden layers and sigmoid activation function at the last layer decreases drastically to 11% in comparison to the actual linear model provided as part of exercise solution .\n",
        "\n",
        "> The accuracy of MLP with 1 hidden layer  is 72.6% which is better than a more  complex model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByeTri7jBNZp",
        "colab_type": "text"
      },
      "source": [
        "**Custom function for MLP Network** accepting paramters for layer dimensions, epochs, learning rate and activation functions created.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVt_lN7WLAif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#Network parameters\n",
        "input_layer = 784    #24x24 images flattened\n",
        "output_layer = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "class Network:  \n",
        "  def update_weights(self, index, gradient):\n",
        "        self.weights[index].assign_sub(self.learning_rate * gradient)\n",
        "        \n",
        "  def update_bias(self, index, gradient):\n",
        "        self.bias[index].assign_sub(self.learning_rate * gradient)        \n",
        "        \n",
        "  def __init__(self, dimensions, learning_rate, activation_function):\n",
        "       # \"dimensions\" : dimension of the network. (input, hidden layers, output)\n",
        "       # \"activations\" : for activation funtion \n",
        "      \n",
        "    self.dimen = dimensions\n",
        "    self.no_of_layers = len(dimensions)\n",
        "    self.loss = None\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    # Weights and bias \n",
        "    self.weights = {}\n",
        "    self.bias = {}\n",
        "    self.activation = activation_function\n",
        "\n",
        "    for i in range(len(dimensions) - 1):\n",
        "      self.bias[i + 1] = tf.Variable(np.zeros(dimensions[i + 1], dtype=np.float32))# bias initialised to zero\n",
        "      self.weights[i + 1] = tf.Variable(tf.random.normal([dimensions[i], dimensions[i + 1]], 0, .1)) #weights initialised to random normal between -1 and 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUnwe0OQLM7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, train_steps): \n",
        "  for step in range(train_steps):\n",
        "      img_batch, lbl_batch = data.next_batch()\n",
        "      parameters =[]\n",
        "      with tf.GradientTape() as tape:\n",
        "          hidden_layer = img_batch #input layer\n",
        "          for layer in range(len(net.dimen) - 2): #2 for input and output layer\n",
        "              if(net.activation == \"tanh\"):\n",
        "                hidden_layer = tf.nn.tanh(tf.matmul(hidden_layer, net.weights[layer+1]) + net.bias[layer+1])\n",
        "              else:\n",
        "                hidden_layer = tf.nn.relu(tf.matmul(hidden_layer, net.weights[layer+1]) + net.bias[layer+1])\n",
        "              parameters.append(net.weights[layer+1]) \n",
        "              parameters.append(net.bias[layer+1])\n",
        "   \n",
        "          last_layer_index = len(net.dimen)-1 # output layer\n",
        "          parameters.append(net.weights[last_layer_index])\n",
        "          parameters.append(net.bias[last_layer_index]) # 10 mnist labels\n",
        "          logits = tf.matmul(hidden_layer, net.weights[last_layer_index]) + net.bias[last_layer_index]\n",
        "          xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=lbl_batch))\n",
        "         \n",
        "      grads = tape.gradient(xent, parameters)\n",
        "      for param_index in range(1,len(parameters),2):\n",
        "        net.update_weights(param_index//2 + 1,grads[param_index-1]) #first param for W\n",
        "        net.update_bias(param_index//2 + 1,grads[param_index]) #second param for b\n",
        "\n",
        "        if not step % 100:\n",
        "          preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "          acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),tf.float32))\n",
        "          print(\"epoch:{} / {}\".format(step,train_steps)+\" Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XJGgbtfsG1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net):\n",
        "    hidden_layer = data.test_data \n",
        "    for layer in range(len(net.dimen) - 2): \n",
        "      hidden_layer = tf.nn.relu(tf.matmul(hidden_layer, net.weights[layer+1]) + net.bias[layer+1])\n",
        "           \n",
        "    last_layer_index = len(net.dimen)-1 # output layer\n",
        "    logits = tf.nn.sigmoid(tf.matmul(hidden_layer, net.weights[last_layer_index]) + net.bias[last_layer_index])\n",
        "    test_preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "\n",
        "    acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),tf.float32))\n",
        "    print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RugSdXlo5kII",
        "colab_type": "text"
      },
      "source": [
        "Single- hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieTP6xpG5d66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "6a5ba98f-df82-44fc-e9d9-4b84a0b89958"
      },
      "source": [
        "learning_rate = 0.1\n",
        "net = Network((784,256, 10), learning_rate, \"relu\")\n",
        "train(net, 1000)\n",
        "test(net)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 / 1000 Loss: 2.3026344776153564 Accuracy: 0.1328125\n",
            "epoch:0 / 1000 Loss: 2.3026344776153564 Accuracy: 0.1328125\n",
            "epoch:100 / 1000 Loss: 2.302067279815674 Accuracy: 0.0859375\n",
            "epoch:100 / 1000 Loss: 2.302067279815674 Accuracy: 0.0859375\n",
            "epoch:200 / 1000 Loss: 2.29679274559021 Accuracy: 0.1171875\n",
            "epoch:200 / 1000 Loss: 2.29679274559021 Accuracy: 0.1171875\n",
            "epoch:300 / 1000 Loss: 2.295921564102173 Accuracy: 0.1171875\n",
            "epoch:300 / 1000 Loss: 2.295921564102173 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "epoch:400 / 1000 Loss: 2.2940824031829834 Accuracy: 0.1328125\n",
            "epoch:400 / 1000 Loss: 2.2940824031829834 Accuracy: 0.1328125\n",
            "epoch:500 / 1000 Loss: 2.2877655029296875 Accuracy: 0.0859375\n",
            "epoch:500 / 1000 Loss: 2.2877655029296875 Accuracy: 0.0859375\n",
            "epoch:600 / 1000 Loss: 2.2941079139709473 Accuracy: 0.1015625\n",
            "epoch:600 / 1000 Loss: 2.2941079139709473 Accuracy: 0.1015625\n",
            "epoch:700 / 1000 Loss: 2.289069414138794 Accuracy: 0.125\n",
            "epoch:700 / 1000 Loss: 2.289069414138794 Accuracy: 0.125\n",
            "epoch:800 / 1000 Loss: 2.2899527549743652 Accuracy: 0.140625\n",
            "epoch:800 / 1000 Loss: 2.2899527549743652 Accuracy: 0.140625\n",
            "Starting new epoch...\n",
            "epoch:900 / 1000 Loss: 2.2991786003112793 Accuracy: 0.1015625\n",
            "epoch:900 / 1000 Loss: 2.2991786003112793 Accuracy: 0.1015625\n",
            "Test accuracy: 0.10360000282526016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNl95jk4-n21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "1c7bfa16-74cb-45e0-a4a0-483c72a58bef"
      },
      "source": [
        "learning_rate = 0.1\n",
        "net = Network((784,256,128, 10), learning_rate, \"relu\")\n",
        "train(net, 1000)\n",
        "test(net)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 / 1000 Loss: 2.3022518157958984 Accuracy: 0.15625\n",
            "epoch:0 / 1000 Loss: 2.3022518157958984 Accuracy: 0.15625\n",
            "epoch:0 / 1000 Loss: 2.3022518157958984 Accuracy: 0.15625\n",
            "epoch:100 / 1000 Loss: 2.2936315536499023 Accuracy: 0.125\n",
            "epoch:100 / 1000 Loss: 2.2936315536499023 Accuracy: 0.125\n",
            "epoch:100 / 1000 Loss: 2.2936315536499023 Accuracy: 0.125\n",
            "epoch:200 / 1000 Loss: 2.2980496883392334 Accuracy: 0.0625\n",
            "epoch:200 / 1000 Loss: 2.2980496883392334 Accuracy: 0.0625\n",
            "epoch:200 / 1000 Loss: 2.2980496883392334 Accuracy: 0.0625\n",
            "epoch:300 / 1000 Loss: 2.296198606491089 Accuracy: 0.1640625\n",
            "epoch:300 / 1000 Loss: 2.296198606491089 Accuracy: 0.1640625\n",
            "epoch:300 / 1000 Loss: 2.296198606491089 Accuracy: 0.1640625\n",
            "Starting new epoch...\n",
            "epoch:400 / 1000 Loss: 2.2984964847564697 Accuracy: 0.1171875\n",
            "epoch:400 / 1000 Loss: 2.2984964847564697 Accuracy: 0.1171875\n",
            "epoch:400 / 1000 Loss: 2.2984964847564697 Accuracy: 0.1171875\n",
            "epoch:500 / 1000 Loss: 2.282299518585205 Accuracy: 0.15625\n",
            "epoch:500 / 1000 Loss: 2.282299518585205 Accuracy: 0.15625\n",
            "epoch:500 / 1000 Loss: 2.282299518585205 Accuracy: 0.15625\n",
            "epoch:600 / 1000 Loss: 2.29559588432312 Accuracy: 0.1328125\n",
            "epoch:600 / 1000 Loss: 2.29559588432312 Accuracy: 0.1328125\n",
            "epoch:600 / 1000 Loss: 2.29559588432312 Accuracy: 0.1328125\n",
            "epoch:700 / 1000 Loss: 2.2946128845214844 Accuracy: 0.0703125\n",
            "epoch:700 / 1000 Loss: 2.2946128845214844 Accuracy: 0.0703125\n",
            "epoch:700 / 1000 Loss: 2.2946128845214844 Accuracy: 0.0703125\n",
            "Starting new epoch...\n",
            "epoch:800 / 1000 Loss: 2.28916072845459 Accuracy: 0.1171875\n",
            "epoch:800 / 1000 Loss: 2.28916072845459 Accuracy: 0.1171875\n",
            "epoch:800 / 1000 Loss: 2.28916072845459 Accuracy: 0.1171875\n",
            "epoch:900 / 1000 Loss: 2.300276517868042 Accuracy: 0.078125\n",
            "epoch:900 / 1000 Loss: 2.300276517868042 Accuracy: 0.078125\n",
            "epoch:900 / 1000 Loss: 2.300276517868042 Accuracy: 0.078125\n",
            "Test accuracy: 0.1826000064611435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIsEdO0Mw5So",
        "colab_type": "text"
      },
      "source": [
        "To experiment i am using keras layers instead of custom function onwards:\n",
        "MLP with 1 layer , relu activation function and Mean Squared error as loss function : 11.9% test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzZROYMOtoyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "efc078b4-6a4b-4195-a827-3a8866b6c31f"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3055 - accuracy: 0.1282\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3051 - accuracy: 0.1310\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3050 - accuracy: 0.1261\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3049 - accuracy: 0.1231\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3049 - accuracy: 0.1224\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1221\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1225\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1225\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1220\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3048 - accuracy: 0.1216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f446fbdcb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob_ZixIevGZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9875a131-90fa-4ec0-bb31-41a3c35e48db"
      },
      "source": [
        "model.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 27.2433 - accuracy: 0.1193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27.243349075317383, 0.1193000003695488]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG-walxfxTCl",
        "colab_type": "text"
      },
      "source": [
        "MLP with 2 layers , relu activation function and Mean Squared error as loss function : 11.5% test accuracy\n",
        "\n",
        "> Takeaway : Model performance does not improves , so next I will try using different loss function to optimize \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-k-5NKDxPvj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1ccb7fc9-9e3f-4798-a537-d8341362526e"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3050 - accuracy: 0.1138\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1112\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1104\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1112\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3046 - accuracy: 0.1124\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3046 - accuracy: 0.1138\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3046 - accuracy: 0.1140\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3046 - accuracy: 0.1141\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1145\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 27.3047 - accuracy: 0.1149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f446d081ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Q1_jF4xRxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fb700442-a742-41c9-b447-c6033706370a"
      },
      "source": [
        "model.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 2ms/step - loss: 27.2432 - accuracy: 0.1150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27.243179321289062, 0.11500000208616257]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jde8I-1dnypT",
        "colab_type": "text"
      },
      "source": [
        "Keras MLP implementation with relu and softmax activation function and Sparse Categorical Cross Entropy : 97.7 % test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n73ayEqamtsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxEnS3pPmzk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5b044bcb-0efc-42c6-c283-cb814de4b4d0"
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5685 - accuracy: 0.9088\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5152 - accuracy: 0.9511\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5013 - accuracy: 0.9636\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4938 - accuracy: 0.9702\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4883 - accuracy: 0.9753\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4849 - accuracy: 0.9787\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4819 - accuracy: 0.9812\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4793 - accuracy: 0.9836\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4778 - accuracy: 0.9846\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4758 - accuracy: 0.9868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4479ce2400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePJwzRVQm2MX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "87b8c6b2-8d56-4e89-c5cc-fd9b3332f0f0"
      },
      "source": [
        "model.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4855 - accuracy: 0.9768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4854873418807983, 0.9768000245094299]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Thzhv8roQgv",
        "colab_type": "text"
      },
      "source": [
        "Keras MLP with 2 hidden layers using relu activation function and Sparse Categorical Cross Entropy as the loss function: 97.9% test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQSGMZu2oN2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8f2446c7-5629-4dcd-d92d-07c01e1adbfc"
      },
      "source": [
        "model1 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='sigmoid')\n",
        "])\n",
        "\n",
        "model1.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model1.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5477 - accuracy: 0.9173\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4962 - accuracy: 0.9600\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4868 - accuracy: 0.9702\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4817 - accuracy: 0.9754\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.4787 - accuracy: 0.9790\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4761 - accuracy: 0.9828\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4747 - accuracy: 0.9839\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4734 - accuracy: 0.9854\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4719 - accuracy: 0.9872\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.4718 - accuracy: 0.9875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4477a3c630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A68sTq4tojQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ddf51d65-6094-4475-f3b4-72f6326688e1"
      },
      "source": [
        "model1.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4793 - accuracy: 0.9794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4792577028274536, 0.9793999791145325]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WGB74ncbVsp",
        "colab_type": "text"
      },
      "source": [
        "Multilayer Perceptron on Fashion MNIST\n",
        "\n",
        "\n",
        "> Use Sparse Categorical Cross Entropy loss function as it worked better for mnist dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ioBG5zTOw5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b8391bb1-262a-4b8c-d59c-cd6c1b3f3268"
      },
      "source": [
        "from datasets import MNISTDataset\n",
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "print(train_labels[0])\n",
        "plt.imshow(train_images[0], cmap=\"Greys_r\")\n",
        "\n",
        "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
        "                    test_images.reshape([-1, 784]), test_labels,\n",
        "                    batch_size=128)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YttPNqXwpHcv",
        "colab_type": "text"
      },
      "source": [
        "MLP Model with 1 Hidden layer and relu activation function : 86% test accuracy\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqgkkaZ7b0vj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8dd91fe4-2f46-4c54-8b09-5ad71d7a734c"
      },
      "source": [
        "fmnist_mlp_model_1 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_1.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_1.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6662 - accuracy: 0.8051\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6201 - accuracy: 0.8435\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6108 - accuracy: 0.8523\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6028 - accuracy: 0.8598\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5975 - accuracy: 0.8644\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5934 - accuracy: 0.8687\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5882 - accuracy: 0.8739\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5856 - accuracy: 0.8767\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5831 - accuracy: 0.8789\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5799 - accuracy: 0.8816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f447da3f400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwmKj28tlgB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "460e6858-712c-4c10-b4d6-d00d841c6a82"
      },
      "source": [
        "fmnist_mlp_model_1.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 2ms/step - loss: 1.5966 - accuracy: 0.8644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5966033935546875, 0.8644000291824341]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ann7lVDkpTzG",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 2 hidden layers with relu activation function : \n",
        "\n",
        "> Layer 1 : 128 units,relu\n",
        "\n",
        "\n",
        "> Layer 2 : 64 units,relu\n",
        "\n",
        "> 10 epochs : 86% test accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkX0e7AYl8ce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4a46c381-b7c5-4af0-8075-562451e4ffe9"
      },
      "source": [
        "fmnist_mlp_model_2 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_2.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_2.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7407 - accuracy: 0.7240\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6476 - accuracy: 0.8151\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6118 - accuracy: 0.8495\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6040 - accuracy: 0.8569\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5973 - accuracy: 0.8637\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5933 - accuracy: 0.8677\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5892 - accuracy: 0.8718\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5868 - accuracy: 0.8742\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5858 - accuracy: 0.8749\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5842 - accuracy: 0.8768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f445bc0b4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXtOEVLRp14M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac055ee2-eeda-4cbd-e3a8-32187337f5e1"
      },
      "source": [
        "fmnist_mlp_model_2.evaluate(test_images,  test_labels)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.6009 - accuracy: 0.8596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6008858680725098, 0.8596000075340271]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wfwtaYmqZkR",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 1 layer and tanh activation function : 87% test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__5Lo6Rp_Fd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "02d88830-d36f-432f-e314-ce73208d3b53"
      },
      "source": [
        "fmnist_mlp_model_3 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='tanh'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_3.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_3.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.7138 - accuracy: 0.7547\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6693 - accuracy: 0.7940\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6259 - accuracy: 0.8388\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5963 - accuracy: 0.8690\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5901 - accuracy: 0.8738\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5831 - accuracy: 0.8821\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5804 - accuracy: 0.8842\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5763 - accuracy: 0.8878\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5728 - accuracy: 0.8911\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5709 - accuracy: 0.8931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f44740120f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGij9-FkqlMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fmnist_mlp_model_3.evaluate(test_images, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGslJ63S_Rl9",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 2 layers and relu activation function :\n",
        "\n",
        "> Layer 1 : 128 units, relu \n",
        "\n",
        "\n",
        "> Layer 2 : 64 units, relu\n",
        "\n",
        "> 10 epochs :  86% test accuracy\n",
        "\n",
        "> 30 epochs : 81% test accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69pI-Sdu-gKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "91d1d2c2-e789-499f-e07c-b5d82b74b48c"
      },
      "source": [
        "fmnist_mlp_model_4 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_4.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "fmnist_mlp_model_4.fit(train_images, train_labels, epochs=30)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7461 - accuracy: 0.7183\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6763 - accuracy: 0.7850\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6694 - accuracy: 0.7911\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6645 - accuracy: 0.7962\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6576 - accuracy: 0.8035\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6590 - accuracy: 0.8014\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6525 - accuracy: 0.8084\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6540 - accuracy: 0.8066\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6515 - accuracy: 0.8092\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6501 - accuracy: 0.8104\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6472 - accuracy: 0.8135\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6486 - accuracy: 0.8122\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6476 - accuracy: 0.8132\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6492 - accuracy: 0.8116\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6469 - accuracy: 0.8138\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6466 - accuracy: 0.8139\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6474 - accuracy: 0.8134\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6436 - accuracy: 0.8173\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6435 - accuracy: 0.8173\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6434 - accuracy: 0.8176\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6469 - accuracy: 0.8138\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6485 - accuracy: 0.8126\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6472 - accuracy: 0.8139\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6465 - accuracy: 0.8144\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6468 - accuracy: 0.8140\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6442 - accuracy: 0.8167\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6470 - accuracy: 0.8140\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6518 - accuracy: 0.8090\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6490 - accuracy: 0.8119\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6452 - accuracy: 0.8157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f44f080be80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NevQkw3j_190",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8c07171f-18df-4736-cc8d-d686271e7468"
      },
      "source": [
        "fmnist_mlp_model_4.evaluate(test_images, test_labels)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.6600 - accuracy: 0.8010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6600499153137207, 0.8009999990463257]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtj2j_3DqwQI",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 2 layers and tanh activation function :\n",
        "\n",
        "> Layer 1 : 128 units, tanh \n",
        "\n",
        "\n",
        "> Layer 2 : 64 units, tanh\n",
        "\n",
        "> 10 epochs :  86% test accuracy\n",
        "\n",
        "> 30 epochs : 86% test accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VMR3EEpqphI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc8be47e-69bf-452f-8b4d-a4fea5502c7a"
      },
      "source": [
        "fmnist_mlp_model_5 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='tanh'),\n",
        "    keras.layers.Dense(64, activation='tanh'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_5.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_5.fit(train_images, train_labels, epochs=30)\n",
        "\n",
        "fmnist_mlp_model_5.evaluate(test_images, test_labels)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6594 - accuracy: 0.8101\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6153 - accuracy: 0.8479\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6070 - accuracy: 0.8553\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5980 - accuracy: 0.8644\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5945 - accuracy: 0.8676\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5889 - accuracy: 0.8727\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5858 - accuracy: 0.8759\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5842 - accuracy: 0.8776\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5815 - accuracy: 0.8805\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5795 - accuracy: 0.8823\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5787 - accuracy: 0.8831\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5784 - accuracy: 0.8831\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5755 - accuracy: 0.8859\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5749 - accuracy: 0.8866\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5744 - accuracy: 0.8869\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5738 - accuracy: 0.8874\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5726 - accuracy: 0.8890\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5725 - accuracy: 0.8888\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5720 - accuracy: 0.8894\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5710 - accuracy: 0.8903\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5728 - accuracy: 0.8884\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5692 - accuracy: 0.8920\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5703 - accuracy: 0.8913\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5690 - accuracy: 0.8923\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5688 - accuracy: 0.8927\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5682 - accuracy: 0.8929\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5669 - accuracy: 0.8944\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5689 - accuracy: 0.8921\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5663 - accuracy: 0.8952\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5660 - accuracy: 0.8953\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.5942 - accuracy: 0.8672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5942496061325073, 0.8672000169754028]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnxgRrWUq9wU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "37bac900-bf82-4045-8d98-be3df842bf8b"
      },
      "source": [
        "fmnist_mlp_model_5.evaluate(test_images, test_labels)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.5942 - accuracy: 0.8672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5942496061325073, 0.8672000169754028]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UToP86Nrc4y",
        "colab_type": "text"
      },
      "source": [
        "MLP model with 2 dense layers, one with relu and second layer with tanh activation : \n",
        "\n",
        "> Layer 1 : 128 units, relu\n",
        "\n",
        "> Layer 2 : 128 units, tanh\n",
        "\n",
        "> 10 epochs : 86.4% test accuracy\n",
        "\n",
        "> 30 epochs : 88% test accuracy\n",
        "\n",
        "> Takeaway: using different activation functions within the same model does not improve model performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw0HYBEArDrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "820f7108-daf4-4083-eb1f-10ef23bb2e6a"
      },
      "source": [
        "fmnist_mlp_model_6 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='tanh'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "fmnist_mlp_model_6.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fmnist_mlp_model_6.fit(train_images, train_labels, epochs=30)\n",
        "\n",
        "fmnist_mlp_model_6.evaluate(test_images, test_labels)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6627 - accuracy: 0.8042\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6149 - accuracy: 0.8463\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6058 - accuracy: 0.8554\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5988 - accuracy: 0.8622\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5932 - accuracy: 0.8684\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5904 - accuracy: 0.8701\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5865 - accuracy: 0.8743\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5833 - accuracy: 0.8778\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5828 - accuracy: 0.8779\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5789 - accuracy: 0.8819\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5755 - accuracy: 0.8853\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5741 - accuracy: 0.8866\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5736 - accuracy: 0.8870\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5712 - accuracy: 0.8896\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5712 - accuracy: 0.8897\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5699 - accuracy: 0.8910\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5690 - accuracy: 0.8919\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5672 - accuracy: 0.8939\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5647 - accuracy: 0.8965\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5661 - accuracy: 0.8955\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5646 - accuracy: 0.8963\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5636 - accuracy: 0.8973\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5618 - accuracy: 0.8994\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5623 - accuracy: 0.8981\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5598 - accuracy: 0.9011\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5590 - accuracy: 0.9022\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5586 - accuracy: 0.9024\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5587 - accuracy: 0.9024\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5582 - accuracy: 0.9027\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.5566 - accuracy: 0.9043\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.5795 - accuracy: 0.8814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5794687271118164, 0.8813999891281128]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy3t-SqF5F18",
        "colab_type": "text"
      },
      "source": [
        "Observations :\n",
        "\n",
        "> Random initialization of weights : Weights initialized to zero will output results as if they are the linear combination of weights which are eventually learned over inputs through various layers. So, our output will remain symmetric to our input as during forward propagation and backpropagation our weights will be identical/symmetric to the weights of previous layers. Hence, In order to truly learn we want the weights to be random.\n",
        "\n",
        "> Epochs : Training the model for more epochs improved accuracy.\n",
        "\n",
        "> Activation Function : RELU activation function gave better accuracy for model as compared to tanh. Mixing activation function like relu and tanh within 2 layer MLP also improved the accuracy.\n",
        "\n",
        "> Loss Function : Changing the loss function from mean squared error to sparse cross entropy improved accuracy. MSE loss function does not work great for high dimensional data like images.\n",
        "\n",
        "> Change in layer dimensions for 2-layer model: 2-layer models in which layers dimension was reduced to half in the second layer didn't improve the accuracy even after executing for more epochs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfiVPeIj7lTf",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "\n",
        "1.   http://blog.ai.ovgu.de/posts/jens/2019/002_tf20_basic_mnist/index.html\n",
        "2.   https://www.tensorflow.org/tutorials/keras/classification\n",
        "3.   https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zk50ZJrrcJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}