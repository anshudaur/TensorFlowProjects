{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshudaur/TensorFlowProjects/blob/master/MNIST_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7et3lTtTttSQ",
        "colab_type": "code",
        "outputId": "636e23ea-a048-47d3-c43e-60d8c6b22397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKdx11ObFdRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x is input - MNIST dataset[55000,784]\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "#reshaped for conv and pool layer which takes 4d input (#training_examples,height,width,#channels)\n",
        "#mnist imgs are 28,28 size and gray scaled so #channels = 1\n",
        "x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "# y will have output of the model, one-hot-encoded output for 0-9\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# Training, Y_ holds correct labels\n",
        "y_ = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# cross_entropy as one of the loss functions\n",
        "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "\n",
        "#GradientDescent\n",
        "#train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
        "\n",
        "def conv_layer(input_data, num_channels,num_filters,filter_shape, pool_shape, name):\n",
        "    #convolution layer filter shape, filter_shape[0] == filter_shape[1]\n",
        "    #conv2d function : 4 arguments \n",
        "    #filter height,width,#inputchannels, #outputchannels\n",
        "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_channels,num_filters]\n",
        "\n",
        "    # initialise weights and bias with random values from normal distribution for the filter\n",
        "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),name=name+'_W')\n",
        "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
        "\n",
        "    # convolutional layer operation\n",
        "    # [1, 1, 1, 1] : stride of 1,1 in x and y direction\n",
        "    output_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "    # add the bias\n",
        "    output_layer += bias\n",
        "\n",
        "    # apply ReLU non-linear activation\n",
        "    output_layer = tf.nn.relu(output_layer)\n",
        "\n",
        "    # now perform max pooling in x and y direction\n",
        "    # x direction = pool_shape[0] similiar for y\n",
        "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
        "    strides = [1, 2, 2, 1]\n",
        "    output_layer = tf.nn.max_pool(output_layer, ksize=ksize, strides=strides,padding='SAME')\n",
        "\n",
        "    return output_layer\n",
        "\n",
        "#CNN Layers\n",
        "# create some convolutional layers with convolution output of : \n",
        "#height/width= int((height/width - filter + 2 * pad) / stride) + 1\n",
        "#stride = 2\n",
        "layer1 = conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name='layer1')\n",
        "\n",
        "#layer 2 :  inputdata = 14*14, #channels = 32 )\n",
        "layer2 = conv_layer(layer1, 32, 64, [5, 5], [2, 2], name='layer2')\n",
        "\n",
        "#FullyConnected(FC) layers : accept flattened inputs \n",
        "# 7*7 input size and 64 channels \n",
        "flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])\n",
        "\n",
        "# weights and bias values for FC layer\n",
        "wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03), name='wd1')\n",
        "bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
        "dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
        "\n",
        "#RELU as non linear activation function for FC layer output\n",
        "dense_layer1 = tf.nn.relu(dense_layer1)\n",
        "\n",
        "# Final FC layer with softmax activations\n",
        "wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2')\n",
        "bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2')\n",
        "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
        "y = tf.nn.softmax(dense_layer2)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71g8DmFuuRp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters\n",
        "learning_rate = 0.0001\n",
        "epochs = 10\n",
        "batch_size = 50\n",
        "\n",
        "\n",
        "# GRADED FUNCTION: compute_cost \n",
        "\n",
        "def compute_cost(Z, Y):\n",
        "    #computes cost\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y))\n",
        "\n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDp0h4x9sPMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "a7d74505-2b26-4c86-fd77-be3c6e0c3bb2"
      },
      "source": [
        "cost = compute_cost(y_,y)\n",
        "\n",
        "# add an optimiser\n",
        "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# define an accuracy assessment operation\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "'''\n",
        "# setup the initialisation operator\n",
        "init_op = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    # initialise the variables\n",
        "    sess.run(init_op)\n",
        "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        avg_cost = 0\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
        "            _, c = sess.run(optimiser,feed_dict={x: batch_x, y: batch_y})\n",
        "            avg_cost += c / total_batch\n",
        "        test_acc = sess.run(accuracy,feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \"test accuracy: {:.3f}\".format(test_acc))\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))'''\n",
        "\n",
        "#Launch the model\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "#train for 1000 run\n",
        "for _ in range(1000):\n",
        "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
        "  sess.run(optimiser, feed_dict={x: batch_xs, y_: batch_ys})\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-02fa78bac884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# add an optimiser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# define an accuracy assessment operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    408\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
            "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 16) dtype=float32>\", \"<tf.Variable 'conv2d/bias:0' shape=(16,) dtype=float32>\", \"<tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 16, 16) dtype=float32>\", \"<tf.Variable 'conv2d_1/bias:0' shape=(16,) dtype=float32>\", \"<tf.Variable 'dense/kernel:0' shape=(16, 10) dtype=float32>\", \"<tf.Variable 'dense/bias:0' shape=(10,) dtype=float32>\", \"<tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 1, 16) dtype=float32>\", \"<tf.Variable 'conv2d_2/bias:0' shape=(16,) dtype=float32>\", \"<tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 16, 16) dtype=float32>\", \"<tf.Variable 'conv2d_3/bias:0' shape=(16,) dtype=float32>\", \"<tf.Variable 'dense_1/kernel:0' shape=(16, 10) dtype=float32>\", \"<tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32>\", \"<tf.Variable 'layer1_W:0' shape=(5, 5, 1, 32) dtype=float32_ref>\", \"<tf.Variable 'layer1_b:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'layer1_W_1:0' shape=(5, 5, 1, 32) dtype=float32_ref>\", \"<tf.Variable 'layer1_b_1:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'layer2_W:0' shape=(5, 5, 32, 64) dtype=float32_ref>\", \"<tf.Variable 'layer2_b:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'wd1:0' shape=(3136, 1000) dtype=float32_ref>\", \"<tf.Variable 'bd1:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'wd2:0' shape=(1000, 10) dtype=float32_ref>\", \"<tf.Variable 'bd2:..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hz3K6F1x_ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}