{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CVDL-project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshudaur/TensorFlowProjects/blob/master/CVDL_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-NcbAOrSu6G",
        "colab_type": "code",
        "outputId": "e93f549d-5c23-4234-c930-9f62752234af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "#for resnet\n",
        "!pip install tensorflow-probability==0.8.0rc0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-probability==0.8.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/63/f54ce32063abaa682d779e44b49eb63fcf63c2422f978842fdeda794337d/tensorflow_probability-0.8.0rc0-py2.py3-none-any.whl (2.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 8.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 40kB 5.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |█                               | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92kB 11.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |█▍                              | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |██                              | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |██                              | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |██▌                             | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |███                             | 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 245kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 256kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 266kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 276kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 286kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 296kB 9.3MB/s eta 0:00:01\r\u001b[K     |████                            | 307kB 9.3MB/s eta 0:00:01\r\u001b[K     |████                            | 317kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 327kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 337kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 348kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 358kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 368kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 378kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 389kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 399kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 409kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 419kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 430kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 440kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 450kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 460kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 471kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 481kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 491kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 501kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 512kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 522kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 532kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 542kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 552kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 563kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 573kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 583kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 593kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 604kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 614kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 624kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 634kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 645kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 655kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 665kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 675kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 686kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 696kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 706kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 716kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 727kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 737kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 747kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 757kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 768kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 778kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 788kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 798kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 808kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 819kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 829kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 839kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 849kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 860kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 870kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 880kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 890kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 901kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 911kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 921kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 931kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 942kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 952kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 962kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 972kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 983kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 993kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.7MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.7MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.7MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.7MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.7MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.7MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.7MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.8MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.9MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.3MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 9.3MB/s \n",
            "\u001b[?25hCollecting cloudpickle==1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/24/fb/4f92f8c0f40a0d728b4f3d5ec5ff84353e705d8ff5e3e447620ea98b06bd/cloudpickle-1.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0rc0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0rc0) (1.17.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0rc0) (4.4.1)\n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 has requirement tensorflow-probability==0.7.0, but you'll have tensorflow-probability 0.8.0rc0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: gym 0.15.4 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.1.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cloudpickle, tensorflow-probability\n",
            "  Found existing installation: cloudpickle 1.2.2\n",
            "    Uninstalling cloudpickle-1.2.2:\n",
            "      Successfully uninstalled cloudpickle-1.2.2\n",
            "  Found existing installation: tensorflow-probability 0.7.0\n",
            "    Uninstalling tensorflow-probability-0.7.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.7.0\n",
            "Successfully installed cloudpickle-1.1.1 tensorflow-probability-0.8.0rc0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apfj4pmLtab5",
        "colab_type": "code",
        "outputId": "916f2945-cfd7-4b4d-a7dd-78c05aceb8dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3lybLMltzq9",
        "colab_type": "code",
        "outputId": "4019882b-54ec-4b62-dfc4-8b4834218a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/CVDL_Project/dataset/'  #change dir to your project folder"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXl5wXsI14RQ",
        "colab_type": "code",
        "outputId": "6ece26ae-2951-49bb-ca93-cc5fd8344405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "data_dir=pathlib.Path(root_path+'train')\n",
        "valid_dir=pathlib.Path(root_path+'valid')\n",
        "test_dir=pathlib.Path(root_path+'test')\n",
        "print(data_dir,',',valid_dir,',',test_dir)\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)\n",
        "valid_image_count = len(list(valid_dir.glob('*/*.jpg')))\n",
        "print(valid_image_count)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/CVDL_Project/dataset/train , gdrive/My Drive/CVDL_Project/dataset/valid , gdrive/My Drive/CVDL_Project/dataset/test\n",
            "4918\n",
            "1638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynG-2RGj14RV",
        "colab_type": "code",
        "outputId": "ab9bc833-5c43-41aa-c8b5-416042b2ed04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import numpy as np\n",
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
        "print(CLASS_NAMES)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['98' '99' '93' '97' '96' '94' '90' '95' '92' '91' '9' '89' '85' '82' '83'\n",
            " '84' '88' '86' '87' '81' '8' '80' '79' '78' '71' '72' '75' '74' '76' '73'\n",
            " '70' '77' '69' '7' '67' '68' '62' '61' '64' '63' '65' '60' '66' '6' '59'\n",
            " '58' '57' '56' '51' '49' '53' '54' '55' '52' '5' '50' '47' '48' '45' '40'\n",
            " '44' '41' '4' '43' '42' '46' '39' '38' '37' '36' '34' '31' '3' '35' '29'\n",
            " '32' '30' '33' '28' '27' '26' '25' '22' '18' '23' '21' '20' '24' '2' '19'\n",
            " '16' '17' '14' '15' '12' '11' '100' '102' '101' '1' '13' '10']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx-TVrkT14RX",
        "colab_type": "code",
        "outputId": "3990fe1b-4619-4301-a9e7-6a49e685dfd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        " image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
        "                                                                   width_shift_range=0.1,  # Horizontal shift (10% of total width)\n",
        "                                                                   height_shift_range=0.1,  # Vertical shift (10% of total height)\n",
        "                                                                   horizontal_flip=True,\n",
        "                                                                   zoom_range=0.5,        \n",
        "                                                                   rotation_range=45)\n",
        " train_data_gen = image_generator.flow_from_directory(directory=str(data_dir),\n",
        "                                                      #batch_size=BATCH_SIZE,\n",
        "                                                      shuffle=True,\n",
        "                                                      target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                      classes = list(CLASS_NAMES),class_mode='categorical')\n",
        "valid_data_gen = image_generator.flow_from_directory(directory=str(valid_dir),\n",
        "                                                      #batch_size=BATCH_SIZE,\n",
        "                                                      shuffle=True,\n",
        "                                                      target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                      classes = list(CLASS_NAMES),class_mode='categorical')\n",
        "test_data_gen = image_generator.flow_from_directory(directory=str(test_dir),\n",
        "                                                      #batch_size=BATCH_SIZE,\n",
        "                                                      shuffle=True,\n",
        "                                                      target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                      classes = list(CLASS_NAMES),class_mode='categorical')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4918 images belonging to 102 classes.\n",
            "Found 1638 images belonging to 102 classes.\n",
            "Found 1638 images belonging to 102 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUbHiRT_14RZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(image_batch, label_batch):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(25):\n",
        "      ax = plt.subplot(5,5,n+1)\n",
        "      plt.imshow(image_batch[n])\n",
        "      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
        "      plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjV6mTeL14Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image_batch, label_batch = next(train_data_gen)\n",
        "# show_batch(image_batch, label_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5uLiDmi14Re",
        "colab_type": "code",
        "outputId": "7e83fe6f-2813-4d16-9b7c-667358bf3b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
        "test_list_ds = tf.data.Dataset.list_files(str(test_dir/'*/*'))\n",
        "valid_list_ds = tf.data.Dataset.list_files(str(valid_dir/'*/*'))\n",
        "\n",
        "print(type(train_list_ds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH9q_0lO14Rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "     \n",
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  return tf.dtypes.cast(parts[-2] == CLASS_NAMES , tf.int16)\n",
        "  #one_hot_list = one_hot_encode(parts[-2], 102)\n",
        "  '''if parts[-2] == CLASS_NAMES:\n",
        "    label=tf.dtypes.cast(parts[-2],tf.int16)\n",
        "  return tf.one_hot(label,depth=102)'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2ISFe514Rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  # resize the image to the desired size.\n",
        "  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfxBrpoF14Rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  print(label)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ekZGCtD14Rp",
        "colab_type": "code",
        "outputId": "029d73bf-ce64-4020-9d71-2d0fd67ac484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import os\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "train_labeled_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "print(train_labeled_ds)\n",
        "test_labeled_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "print(test_labeled_ds)\n",
        "valid_labeled_ds = valid_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "print(valid_labeled_ds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Cast:0\", shape=(102,), dtype=int16)\n",
            "<DatasetV1Adapter shapes: ((224, 224, 3), (102,)), types: (tf.float32, tf.int16)>\n",
            "Tensor(\"Cast:0\", shape=(102,), dtype=int16)\n",
            "<DatasetV1Adapter shapes: ((224, 224, 3), (102,)), types: (tf.float32, tf.int16)>\n",
            "Tensor(\"Cast:0\", shape=(102,), dtype=int16)\n",
            "<DatasetV1Adapter shapes: ((224, 224, 3), (102,)), types: (tf.float32, tf.int16)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOSU2URL14Rs",
        "colab_type": "code",
        "outputId": "a515059d-0362-4738-da85-e02d0b0d2f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for image, label in train_labeled_ds.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape:  (224, 224, 3)\n",
            "Label:  tf.Tensor(\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(102,), dtype=int16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kbhbf1314Ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_for_training(ds, cache=True, shuffle_buffer_size=6000):\n",
        "  # This is a small dataset, only load it once, and keep it in memory.\n",
        "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
        "  # fit in memory.\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  # Repeat forever\n",
        "  ds = ds.repeat()\n",
        "\n",
        "  ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
        "  # is training.\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKbc9US114Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = prepare_for_training(train_labeled_ds)\n",
        "\n",
        "#image_batch, label_batch = next(iter(train_ds))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBuwGIGnNm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_img_batch, valid_label_batch = next(iter(valid_labeled_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsbW7tyT14Rz",
        "colab_type": "code",
        "outputId": "b232423d-01eb-409d-bd02-3b0bee84e5fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "show_batch(image_batch.numpy(), label_batch.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-126ed6a30af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'image_batch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6WMM7lCHAp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for bilinearcnn\n",
        "def outer_product(x):\n",
        "    \n",
        "    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n",
        "    phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\n",
        "\n",
        "    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n",
        "    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n",
        "    \n",
        "    # Divide by feature map size [sizexsize]\n",
        "    size1 = int(x[1].shape[1])\n",
        "    size2 = int(x[1].shape[2])\n",
        "    phi_I = tf.divide(phi_I, size1*size2)\n",
        "    \n",
        "    # Take signed square root of phi_I\n",
        "    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n",
        "    \n",
        "    # Apply l2 normalization\n",
        "    z_l2 = tf.nn.l2_normalize(y_ssqrt)\n",
        "    return z_l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2HzVFT1MXki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import backend as k\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=102):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0VXLyMC14R1",
        "colab_type": "code",
        "outputId": "4dbbfd8e-ddc6-4f7c-98ff-d16a67da9274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "img_shape = (224, 224, 3)\n",
        "\n",
        "\n",
        "vgg_layer_list = [#layers.Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "              #layers.Conv2D(64, (3,3), activation='relu'),\n",
        "              #layers.MaxPooling2D(),\n",
        "              layers.Conv2D(128, (3,3), activation='relu', input_shape=img_shape),\n",
        "              layers.Conv2D(128, (3,3), activation='relu'),\n",
        "              layers.Conv2D(128, (3,3), activation='relu'),\n",
        "              layers.Conv2D(128, (3,3), activation='relu'),\n",
        "              layers.MaxPooling2D(),\n",
        "              layers.Conv2D(256, (3,3), activation='relu'),\n",
        "              layers.Conv2D(256, (3,3), activation='relu'),\n",
        "              layers.Conv2D(256, (3,3), activation='relu'),\n",
        "              layers.Conv2D(256, (3,3), activation='relu'),\n",
        "              layers.MaxPooling2D(),\n",
        "              layers.Conv2D(512, (3,3), activation='relu'),\n",
        "              layers.Conv2D(512, (3,3), activation='relu'),\n",
        "              layers.Conv2D(512, (3,3), activation='relu'),\n",
        "              layers.Conv2D(512, (3,3), activation='relu'),\n",
        "              layers.MaxPooling2D(),\n",
        "              layers.Conv2D(512, (3,3), activation='relu'),\n",
        "              layers.Conv2D(512, (3,3), activation='relu'),\n",
        "              layers.Conv2D(512, (3,3), activation='relu'),\n",
        "              layers.Conv2D(512, (3,3), activation='relu')]\n",
        "              #layers.MaxPooling2D(),\n",
        "              #layers.Flatten(),layers.Dense(1024),layers.Dense(1024)]\n",
        "              #layers.Dense(102,activation='softmax')]\n",
        "\n",
        "#------------bilinear cnn----------------------------  \n",
        "''' \n",
        "model1 = tf.keras.Sequential(vgg_layer_list)\n",
        "         \n",
        "d1=model1.output\n",
        "d2=model1.output\n",
        "print(type(d1))\n",
        "print(d1.get_shape())\n",
        "x = outer_product([d1,d2])\n",
        "print(x.get_shape())\n",
        "\n",
        "\n",
        "predictions=layers.Dense(102, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = tf.compat.v1.keras.Model(inputs=model1.input, outputs=predictions)'''\n",
        "#------------bilinear cnn---------------------------- \n",
        "\n",
        "\n",
        "#model = tf.keras.Sequential(vgg_layer_list)\n",
        "\n",
        "#learning_rate=1e-4\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "#--------for resnet---------------\n",
        "'''model = resnet_v2(input_shape=img_shape, depth=56)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule(0))'''\n",
        "#--------for resnet---------------"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model = resnet_v2(input_shape=img_shape, depth=56)\\nopt = tf.keras.optimizers.Adam(learning_rate=lr_schedule(0))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITB68uAuYBB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotLosses(history):  \n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "def plotAccuracies(history):  \n",
        "    plt.plot(history.history['accuracy'], label='accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([0.5, 1])\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "def model2(input_shape):  \n",
        "  layer_list = [layers.Conv2D(64, (3,3), activation='relu', input_shape=img_shape),\n",
        "              layers.Conv2D(64, (3,3), activation='relu'),\n",
        "              layers.Conv2D(64, (3,3), activation='relu'),\n",
        "              layers.MaxPooling2D(),\n",
        "              layers.Conv2D(128, (3,3), activation='relu'),\n",
        "              layers.Conv2D(128, (3,3), activation='relu'),\n",
        "              layers.Conv2D(128, (3,3), activation='relu'),\n",
        "              layers.MaxPooling2D(),\n",
        "              layers.Conv2D(256, (5,5), activation='relu'),\n",
        "              layers.Conv2D(256, (5,5), activation='relu'),\n",
        "              layers.MaxPooling2D(),\n",
        "              layers.Conv2D(128, (5,5), activation='relu'),\n",
        "              layers.Conv2D(128, (5,5), activation='relu'),\n",
        "              layers.MaxPooling2D(),\n",
        "              layers.Flatten(),layers.Dense(256,activation='softmax'),\n",
        "              layers.Dropout(0.25),layers.Dense(102)]\n",
        "  model = tf.keras.Sequential(layer_list)\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKAoUnEx14R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import multi_gpu_model\n",
        "def run_model(model):\n",
        "  lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "  lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                                cooldown=0,\n",
        "                                patience=5,\n",
        "                                min_lr=0.5e-6)\n",
        "\n",
        "  callbacks = [lr_reducer, lr_scheduler]\n",
        "\n",
        "  start = time.time()\n",
        "  parallel_model = multi_gpu_model(model, cpu_merge=False,gpus=8)\n",
        "  parallel_model.compile(optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'],callbacks=callbacks)\n",
        "  parallel_model.fit_generator(train_data_gen,BATCH_SIZE=256,\n",
        "                      epochs=10,steps_per_epoch = image_count//BATCH_SIZE,use_multiprocessing=True,\n",
        "                      validation_data=valid_data_gen)\n",
        "\n",
        "  plotLosses(history)\n",
        "  plotAccuracies(history)\n",
        "  end = time.time()\n",
        "  print('Processing time:',(end - start)/60)\n",
        "\n",
        "#model.save_weights(root_path+'resnet_cnn_model_img_generator.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMJL-5foY04Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = (224, 224, 3)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "#model = model2(input_shape,0.001)\n",
        "#run_model(model)\n",
        "\n",
        "#scores = model.evaluate_generator(test_data_gen,workers=12)\n",
        "#print(scores)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRvfJZNWhYR6",
        "colab_type": "code",
        "outputId": "72eb7e9a-18ba-4d3d-d7f3-350ddeb6a634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "model = resnet_v2(input_shape=input_shape, depth=56)\n",
        "run_model(model)\n",
        "#ResourceExhaustedError: OOM when allocating tensor with shape\n",
        "#AttributeError: module 'tensorflow' has no attribute 'get_default_session'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a47d425081ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#ResourceExhaustedError: OOM when allocating tensor with shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#AttributeError: module 'tensorflow' has no attribute 'get_default_session'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-da23625ef208>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mparallel_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_merge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   parallel_model.compile(optimizer=Adam(learning_rate=lr_schedule(0)),\n\u001b[1;32m     18\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/utils/multi_gpu_utils.py\u001b[0m in \u001b[0;36mmulti_gpu_model\u001b[0;34m(model, gpus, cpu_merge, cpu_relocation)\u001b[0m\n\u001b[1;32m    183\u001b[0m                        \u001b[0;34m'However this machine only has: %s. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                        'Try reducing `gpus`.' % (gpus, target_devices,\n\u001b[0;32m--> 185\u001b[0;31m                                                  available_devices))\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: To call `multi_gpu_model` with `gpus=8`, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3', '/gpu:4', '/gpu:5', '/gpu:6', '/gpu:7']. However this machine only has: ['/cpu:0', '/xla_cpu:0', '/xla_gpu:0', '/gpu:0']. Try reducing `gpus`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdqOWs0XoTX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5uZVSaP14R-",
        "colab_type": "code",
        "outputId": "0693d977-9ecd-454f-c014-7b5cd5e9fd79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "\n",
        "model = tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=102)\n",
        "\n",
        "run_model(model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d6e386b23f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseNet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m102\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-d7d6ac327900>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mparallel_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_merge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   parallel_model.compile(optimizer=Adam(learning_rate=lr_schedule(0)),\n\u001b[1;32m     18\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/multi_gpu_utils.py\u001b[0m in \u001b[0;36mmulti_gpu_model\u001b[0;34m(model, gpus, cpu_merge, cpu_relocation)\u001b[0m\n\u001b[1;32m    148\u001b[0m                          'with the TensorFlow backend.')\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mavailable_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_available_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     available_devices = [_normalize_device_name(name)\n\u001b[1;32m    152\u001b[0m                          for name in available_devices]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/multi_gpu_utils.py\u001b[0m in \u001b[0;36m_get_available_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_available_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mdefault_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdefault_session\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_session'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdJvLQPxic4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}